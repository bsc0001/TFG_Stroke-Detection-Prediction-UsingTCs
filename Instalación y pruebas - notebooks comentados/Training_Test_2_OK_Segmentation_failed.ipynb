{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bsc0001/niftynet\n"
     ]
    }
   ],
   "source": [
    "%cd /home/bsc0001/niftynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in ./lib/python3.6/site-packages (1.2.0)\n",
      "Collecting pyyaml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 700kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/bsc0001/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml\n",
      "Successfully installed pyyaml-5.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install SimpleITK\n",
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL:tensorflow:Optional Python module cv2 not found, please install cv2 and retry if the application fails.\n",
      "CRITICAL:tensorflow:Optional Python module skimage.io not found, please install skimage.io and retry if the application fails.\n",
      "NiftyNet version 0.5.0\n",
      "[CUSTOM]\n",
      "-- num_classes: 2\n",
      "-- output_prob: False\n",
      "-- label_normalisation: False\n",
      "-- softmax: True\n",
      "-- min_sampling_ratio: 0\n",
      "-- compulsory_labels: (0, 1)\n",
      "-- rand_samples: 0\n",
      "-- min_numb_labels: 1\n",
      "-- proba_connect: True\n",
      "-- evaluation_units: foreground\n",
      "-- label: ('label',)\n",
      "-- sampler: ()\n",
      "-- image: ('ct',)\n",
      "-- inferred: ()\n",
      "-- weight: ()\n",
      "-- name: net_segment\n",
      "[CONFIG_FILE]\n",
      "-- path: /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/config.ini\n",
      "[CT]\n",
      "-- csv_file: \n",
      "-- path_to_search: ./data/dense_vnet_TC_craneal/\n",
      "-- filename_contains: ('TC',)\n",
      "-- filename_not_contains: ()\n",
      "-- filename_removefromid: \n",
      "-- interp_order: 1\n",
      "-- loader: None\n",
      "-- pixdim: ()\n",
      "-- axcodes: ('A', 'R', 'S')\n",
      "-- spatial_window_size: (512, 512, 40)\n",
      "[LABEL]\n",
      "-- csv_file: \n",
      "-- path_to_search: ./data/dense_vnet_TC_craneal/\n",
      "-- filename_contains: ('mask',)\n",
      "-- filename_not_contains: ()\n",
      "-- filename_removefromid: \n",
      "-- interp_order: 0\n",
      "-- loader: None\n",
      "-- pixdim: ()\n",
      "-- axcodes: ('A', 'R', 'S')\n",
      "-- spatial_window_size: (512, 512, 40)\n",
      "[SYSTEM]\n",
      "-- cuda_devices: \"\"\n",
      "-- num_threads: 3\n",
      "-- num_gpus: 3\n",
      "-- model_dir: /home/bsc0001/niftynet/models/dense_vnet_TC_craneal\n",
      "-- dataset_split_file: ./dataset_split.csv\n",
      "-- event_handler: ('model_saver', 'model_restorer', 'sampler_threading', 'apply_gradients', 'output_interpreter', 'console_logger', 'tensorboard_logger')\n",
      "-- iteration_generator: iteration_generator\n",
      "-- action: training\n",
      "[NETWORK]\n",
      "-- name: dense_vnet_TC_craneal.CNN_TC_craneal_ictus.MyNet\n",
      "-- activation_function: relu\n",
      "-- batch_size: 2\n",
      "-- smaller_final_batch_mode: pad\n",
      "-- decay: 0.0\n",
      "-- reg_type: L2\n",
      "-- volume_padding_size: (0, 0, 0)\n",
      "-- volume_padding_mode: minimum\n",
      "-- window_sampling: resize\n",
      "-- queue_length: 12\n",
      "-- multimod_foreground_type: and\n",
      "-- histogram_ref_file: ./histogram_ref_file.txt\n",
      "-- norm_type: percentile\n",
      "-- cutoff: (0.01, 0.99)\n",
      "-- foreground_type: otsu_plus\n",
      "-- normalisation: False\n",
      "-- whitening: False\n",
      "-- normalise_foreground_only: False\n",
      "-- weight_initializer: he_normal\n",
      "-- bias_initializer: zeros\n",
      "-- keep_prob: 1.0\n",
      "-- weight_initializer_args: {}\n",
      "-- bias_initializer_args: {}\n",
      "[TRAINING]\n",
      "-- optimiser: adam\n",
      "-- sample_per_volume: 1\n",
      "-- rotation_angle: ()\n",
      "-- rotation_angle_x: ()\n",
      "-- rotation_angle_y: ()\n",
      "-- rotation_angle_z: ()\n",
      "-- scaling_percentage: ()\n",
      "-- antialiasing: True\n",
      "-- bias_field_range: ()\n",
      "-- bf_order: 3\n",
      "-- random_flipping_axes: -1\n",
      "-- do_elastic_deformation: False\n",
      "-- num_ctrl_points: 4\n",
      "-- deformation_sigma: 15\n",
      "-- proportion_to_deform: 0.5\n",
      "-- lr: 0.001\n",
      "-- loss_type: dense_vnet_TC_craneal.dice_hinge.dice\n",
      "-- starting_iter: 0\n",
      "-- save_every_n: 100\n",
      "-- tensorboard_every_n: 20\n",
      "-- max_iter: 300\n",
      "-- max_checkpoints: 100\n",
      "-- validation_every_n: -1\n",
      "-- validation_max_iter: 1\n",
      "-- exclude_fraction_for_validation: 0.0\n",
      "-- exclude_fraction_for_inference: 0.0\n",
      "-- vars_to_restore: \n",
      "-- vars_to_freeze: \n",
      "[INFERENCE]\n",
      "-- spatial_window_size: (512, 512, 40)\n",
      "-- inference_iter: 3000\n",
      "-- dataset_to_infer: \n",
      "-- save_seg_dir: ./segmentation_output/\n",
      "-- output_postfix: _niftynet_out\n",
      "-- output_interp_order: 0\n",
      "-- border: (0, 0, 0)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m starting segmentation application\n",
      "\u001b[1mINFO:niftynet:\u001b[0m `csv_file = ` not found, writing to \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/ct.csv\" instead.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Overwriting existing: \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/ct.csv\".\n",
      "\u001b[1mINFO:niftynet:\u001b[0m [ct] search file folders, writing csv file /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/ct.csv\n",
      "\u001b[1mINFO:niftynet:\u001b[0m `csv_file = ` not found, writing to \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/label.csv\" instead.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Overwriting existing: \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/label.csv\".\n",
      "\u001b[1mINFO:niftynet:\u001b[0m [label] search file folders, writing csv file /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/label.csv\n",
      "\u001b[1mINFO:niftynet:\u001b[0m \n",
      "\n",
      "Number of subjects 2, input section names: ['subject_id', 'ct', 'label']\n",
      "-- using all subjects (without data partitioning).\n",
      "\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 2 subjects from sections ('ct',) as input [image]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 2 subjects from sections ('label',) as input [label]\n",
      "\n",
      "system_param:  {'SYSTEM': Namespace(action='training', cuda_devices='\"\"', dataset_split_file='/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/dataset_split.csv', event_handler=('model_saver', 'model_restorer', 'sampler_threading', 'apply_gradients', 'output_interpreter', 'console_logger', 'tensorboard_logger'), iteration_generator='iteration_generator', model_dir='/home/bsc0001/niftynet/models/dense_vnet_TC_craneal', num_gpus=3, num_threads=3), 'NETWORK': Namespace(activation_function='relu', batch_size=2, bias_initializer='zeros', bias_initializer_args={}, cutoff=(0.01, 0.99), decay=0.0, foreground_type='otsu_plus', histogram_ref_file='/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/histogram_ref_file.txt', keep_prob=1.0, multimod_foreground_type='and', name='dense_vnet_TC_craneal.CNN_TC_craneal_ictus.MyNet', norm_type='percentile', normalisation=False, normalise_foreground_only=False, queue_length=12, reg_type='L2', smaller_final_batch_mode='pad', volume_padding_mode='minimum', volume_padding_size=(0, 0, 0), weight_initializer='he_normal', weight_initializer_args={}, whitening=False, window_sampling='resize'), 'TRAINING': Namespace(antialiasing=True, bf_order=3, bias_field_range=(), deformation_sigma=15, do_elastic_deformation=False, exclude_fraction_for_inference=0.0, exclude_fraction_for_validation=0.0, loss_type='dense_vnet_TC_craneal.dice_hinge.dice', lr=0.001, max_checkpoints=100, max_iter=300, num_ctrl_points=4, optimiser='adam', proportion_to_deform=0.5, random_flipping_axes=-1, rotation_angle=(), rotation_angle_x=(), rotation_angle_y=(), rotation_angle_z=(), sample_per_volume=1, save_every_n=100, scaling_percentage=(), starting_iter=0, tensorboard_every_n=20, validation_every_n=-1, validation_max_iter=1, vars_to_freeze='', vars_to_restore=''), 'INFERENCE': Namespace(border=(0, 0, 0), dataset_to_infer='', inference_iter=3000, output_interp_order=0, output_postfix='_niftynet_out', save_seg_dir='/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/segmentation_output', spatial_window_size=(512, 512, 40)), 'CUSTOM': Namespace(compulsory_labels=(0, 1), evaluation_units='foreground', image=('ct',), inferred=(), label=('label',), label_normalisation=False, min_numb_labels=1, min_sampling_ratio=0, name='net_segment', num_classes=2, output_prob=False, proba_connect=True, rand_samples=0, sampler=(), softmax=True, weight=()), 'CONFIG_FILE': Namespace(path='/home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/config.ini')}\n",
      "\n",
      "input_data_param: {'ct': Namespace(axcodes=('A', 'R', 'S'), csv_file='', filename_contains=('TC',), filename_not_contains=(), filename_removefromid='', interp_order=1, loader=None, path_to_search='./data/dense_vnet_TC_craneal/', pixdim=(), spatial_window_size=(512, 512, 40)), 'label': Namespace(axcodes=('A', 'R', 'S'), csv_file='', filename_contains=('mask',), filename_not_contains=(), filename_removefromid='', interp_order=0, loader=None, path_to_search='./data/dense_vnet_TC_craneal/', pixdim=(), spatial_window_size=(512, 512, 40))} \n",
      "\n",
      "\n",
      "Params al crear graph:\n",
      "App:  <niftynet.application.segmentation_application.SegmentationApplication object at 0x7f1c0622f668>\n",
      "Num_GPUs:  3\n",
      "Num_hilos:  3\n",
      "Is training?:  True \n",
      "\n",
      "2019-06-18 10:42:34.374372: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-18 10:42:34.628966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:05:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.74GiB\n",
      "2019-06-18 10:42:34.757466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:06:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.75GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 10:42:34.887242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 2 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.75GiB\n",
      "2019-06-18 10:42:34.890881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 10:42:35.683842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 10:42:35.683895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 10:42:35.683902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 10:42:35.683907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 10:42:35.683912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 10:42:35.684692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:35.843525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:35.999167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m reading size of preprocessed images\n",
      "\u001b[1mINFO:niftynet:\u001b[0m initialised resize sampler {'image': (1, 512, 512, 40, 1, 1), 'image_location': (1, 7), 'label': (1, 512, 512, 40, 1, 1), 'label_location': (1, 7)} \n",
      "\u001b[1mWARNING:niftynet:\u001b[0m From /home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "\n",
      "Should be:\n",
      " <tensorflow.python.ops.init_ops.VarianceScaling object at 0x7f1c06215b70> \n",
      "\n",
      "\n",
      "W_INIT:\n",
      " <tensorflow.python.ops.init_ops.VarianceScaling object at 0x7f1c0622fe80> \n",
      "\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Import [MyNet] from /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/CNN_TC_craneal_ictus.py.\n",
      "\n",
      "Should be:\n",
      " <tensorflow.python.ops.init_ops.VarianceScaling object at 0x7f1c0622ffd0> \n",
      "\n",
      "\u001b[1mINFO:niftynet:\u001b[0m using MyNet\n",
      "2019-06-18 10:42:36.213716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 10:42:36.213857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 10:42:36.213867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 10:42:36.213874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 10:42:36.213880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 10:42:36.213886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 10:42:36.214198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:36.214430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:36.214607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initialising Dataset from 2 subjects...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Import [dice] from /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/dice_hinge.py.\n",
      "2019-06-18 10:42:37.169194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 10:42:37.169381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 10:42:37.169393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 10:42:37.169401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 10:42:37.169407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 10:42:37.169413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 10:42:37.169749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:37.169976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:37.170152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Import [dice] from /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/dice_hinge.py.\n",
      "2019-06-18 10:42:37.949186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 10:42:37.949322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 10:42:37.949334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 10:42:37.949341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 10:42:37.949347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 10:42:37.949353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 10:42:37.949651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:37.949843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:37.950024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Import [dice] from /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/dice_hinge.py.\n",
      "2019-06-18 10:42:39.527281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 10:42:39.527426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 10:42:39.527437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 10:42:39.527444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 10:42:39.527450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 10:42:39.527456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 10:42:39.527771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:39.528000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 10:42:39.528179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m Parameters from random initialisations ...\n",
      "dice[0.872955 0]\n",
      "dice[0.867681563 0.0776090175]\n",
      "dice[0.867681563 0.0776090175]\n",
      "dice[0.772152603 0.0647273585]\n",
      "dice[0.771578372 0.064876087]\n",
      "dice[0.771578372 0.064876087]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 1, loss=5.883581161499023, loss_1=0.9883251190185547, loss_2=0.9883251190185547 (35.228505s)\n",
      "dice[0.863684714 0]\n",
      "dice[0.859713912 0.0797586516]\n",
      "dice[0.846191704 0.0588412806]\n",
      "dice[0.784698486 0]\n",
      "dice[0.740638316 0]\n",
      "dice[0.784114957 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 2, loss=10.58790397644043, loss_1=5.6715312004089355, loss_2=6.012092113494873 (13.934508s)\n",
      "dice[0.862043798 0]\n",
      "dice[0.862043798 0]\n",
      "dice[0.862004519 0.081603]\n",
      "dice[0.775756776 0]\n",
      "dice[0.775756776 0]\n",
      "dice[0.766801476 0.0621938668]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 3, loss=10.59054946899414, loss_1=10.59054946899414, loss_2=0.9987876415252686 (14.452962s)\n",
      "dice[0.869654655 0]\n",
      "dice[0.860213041 0.0817097276]\n",
      "dice[0.864430249 0.0816736519]\n",
      "dice[0.773754299 0]\n",
      "dice[0.767169118 0.0641452894]\n",
      "dice[0.766637206 0.0638793]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 4, loss=5.896147727966309, loss_1=0.9659847021102905, loss_2=5.654714107513428 (15.833377s)\n",
      "dice[0.868272185 0]\n",
      "dice[0.874157965 0]\n",
      "dice[0.791254044 0.0694995373]\n",
      "dice[0.77167654 0]\n",
      "dice[0.765088558 0.0663587749]\n",
      "dice[0.777430832 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 5, loss=5.823023319244385, loss_1=10.59001350402832, loss_2=5.856532096862793 (12.729384s)\n",
      "dice[0.800128698 0]\n",
      "dice[0.874512196 0.0799195841]\n",
      "dice[0.874512196 0.0799195841]\n",
      "dice[0.774305105 0.0753357]\n",
      "dice[0.762069583 0.0670182854]\n",
      "dice[0.762069583 0.0670182854]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 6, loss=5.739639759063721, loss_1=0.9268742799758911, loss_2=0.9268742799758911 (15.400095s)\n",
      "dice[0.882764935 0]\n",
      "dice[0.877591372 0]\n",
      "dice[0.876918793 0]\n",
      "dice[0.776818514 0]\n",
      "dice[0.773548663 0]\n",
      "dice[0.766640902 0.0683457851]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 7, loss=10.587383270263672, loss_1=5.821059226989746, loss_2=10.586398124694824 (14.551494s)\n",
      "dice[0.886915565 0]\n",
      "dice[0.878619254 0.0828978643]\n",
      "dice[0.791560769 0.0698920861]\n",
      "dice[0.766137362 0.0696871728]\n",
      "dice[0.768757403 0]\n",
      "dice[0.779278815 0.0749585]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 8, loss=0.9544682502746582, loss_1=5.799031734466553, loss_2=5.640552043914795 (13.933511s)\n",
      "dice[0.89142108 0]\n",
      "dice[0.799891829 0]\n",
      "dice[0.883205652 0.0820685104]\n",
      "dice[0.766016364 0]\n",
      "dice[0.764802933 0.0709955469]\n",
      "dice[0.779291332 0.074909538]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 9, loss=5.74385929107666, loss_1=5.647562026977539, loss_2=5.778509140014648 (12.937549s)\n",
      "dice[0.833818376 0]\n",
      "dice[0.889330506 0]\n",
      "dice[0.829853475 0.0812962353]\n",
      "dice[0.768694699 0]\n",
      "dice[0.762920737 0.0723656]\n",
      "dice[0.759773135 0.0721535608]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 10, loss=0.8455446362495422, loss_1=10.585493087768555, loss_2=5.773638725280762 (13.777092s)\n",
      "dice[0.893525958 0]\n",
      "dice[0.893124759 0]\n",
      "dice[0.795034945 0.0710604936]\n",
      "dice[0.766154826 0]\n",
      "dice[0.7658512 0]\n",
      "dice[0.785931528 0.0742495656]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 11, loss=0.9435758590698242, loss_1=10.585155487060547, loss_2=10.585180282592773 (13.777692s)\n",
      "dice[0.835824668 0]\n",
      "dice[0.833395064 0.0801084116]\n",
      "dice[0.7997576 0.0693730265]\n",
      "dice[0.765430927 0]\n",
      "dice[0.760081112 0]\n",
      "dice[0.788245738 0.0739929]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 12, loss=0.9707528948783875, loss_1=10.599685668945312, loss_2=5.680522441864014 (13.882504s)\n",
      "dice[0.800662756 0.0693338066]\n",
      "dice[0.83984828 0.0790784881]\n",
      "dice[0.83984828 0.0790784881]\n",
      "dice[0.793714821 0]\n",
      "dice[0.754924834 0.0809698552]\n",
      "dice[0.754924834 0.0809698552]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 13, loss=5.819176197052002, loss_1=0.761258602142334, loss_2=0.761258602142334 (15.416239s)\n",
      "dice[0.846703947 0]\n",
      "dice[0.839657664 0.0789508894]\n",
      "dice[0.837494493 0.0818279]\n",
      "dice[0.759207 0.0816922411]\n",
      "dice[0.758247137 0]\n",
      "dice[0.756150723 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 14, loss=5.692076206207275, loss_1=5.663163661956787, loss_2=5.661892414093018 (16.447157s)\n",
      "dice[0.844930828 0]\n",
      "dice[0.849825084 0]\n",
      "dice[0.843702912 0.078308709]\n",
      "dice[0.747690856 0]\n",
      "dice[0.758430183 0.082429558]\n",
      "dice[0.754617453 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 15, loss=5.698470592498779, loss_1=10.601844787597656, loss_2=5.6545090675354 (12.816298s)\n",
      "dice[0.846799135 0]\n",
      "dice[0.852974534 0]\n",
      "dice[0.847063899 0.0778659582]\n",
      "dice[0.745010257 0]\n",
      "dice[0.752502263 0]\n",
      "dice[0.757490873 0.0834535286]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 16, loss=5.703120708465576, loss_1=10.60204792022705, loss_2=5.644967079162598 (16.180247s)\n",
      "dice[0.855757833 0]\n",
      "dice[0.855757833 0]\n",
      "dice[0.850102603 0.080000259]\n",
      "dice[0.756625295 0.0844171122]\n",
      "dice[0.756625295 0.0844171122]\n",
      "dice[0.748971403 0.0807782412]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 17, loss=5.6365065574646, loss_1=5.6365065574646, loss_2=0.7524033784866333 (16.316967s)\n",
      "dice[0.854497552 0]\n",
      "dice[0.807747424 0]\n",
      "dice[0.854497552 0]\n",
      "dice[0.744526207 0]\n",
      "dice[0.744526207 0]\n",
      "dice[0.799706 0.0721743256]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 18, loss=5.773660182952881, loss_1=10.600244522094727, loss_2=10.600244522094727 (12.267521s)\n",
      "dice[0.807138145 0]\n",
      "dice[0.852392316 0]\n",
      "dice[0.854814053 0.0798242167]\n",
      "dice[0.740525723 0]\n",
      "dice[0.801735401 0.0719139725]\n",
      "dice[0.748836 0.082544215]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 19, loss=5.7770094871521, loss_1=0.7364370822906494, loss_2=10.601770401000977 (15.655263s)\n",
      "dice[0.857372344 0]\n",
      "dice[0.808823586 0]\n",
      "dice[0.857372344 0]\n",
      "dice[0.754978836 0.086388208]\n",
      "dice[0.754978836 0.086388208]\n",
      "dice[0.80401063 0.0716673]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 20, loss=5.779560089111328, loss_1=5.621635437011719, loss_2=5.621635437011719 (20.659785s)\n",
      "dice[0.855905414 0.0775799826]\n",
      "dice[0.804137588 0.0671350583]\n",
      "dice[0.855905414 0.0775799826]\n",
      "dice[0.749826491 0]\n",
      "dice[0.749826491 0]\n",
      "dice[0.806497633 0.0713249817]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 21, loss=1.0383164882659912, loss_1=5.704836368560791, loss_2=5.704836368560791 (13.516103s)\n",
      "dice[0.858657241 0]\n",
      "dice[0.858819366 0.0800661296]\n",
      "dice[0.861430764 0.0772088543]\n",
      "dice[0.752807438 0]\n",
      "dice[0.759769261 0.0835721567]\n",
      "dice[0.75752759 0.0862627253]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 22, loss=10.59713363647461, loss_1=0.7314298152923584, loss_2=0.7212516069412231 (14.016447s)\n",
      "dice[0.860939622 0]\n",
      "dice[0.860939622 0]\n",
      "dice[0.861707 0.0783258528]\n",
      "dice[0.75502342 0]\n",
      "dice[0.75502342 0]\n",
      "dice[0.752598405 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 23, loss=10.596009254455566, loss_1=5.694284439086914, loss_2=10.596009254455566 (14.597986s)\n",
      "dice[0.862097144 0]\n",
      "dice[0.865230262 0]\n",
      "dice[0.807051122 0.067329362]\n",
      "dice[0.757504106 0]\n",
      "dice[0.816885769 0]\n",
      "dice[0.769274354 0.0870731622]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 24, loss=5.844025611877441, loss_1=10.595100402832031, loss_2=5.6113810539245605 (16.269450s)\n",
      "dice[0.86577487 0]\n",
      "dice[0.861218 0.0826967]\n",
      "dice[0.865647852 0.0798314139]\n",
      "dice[0.761273444 0]\n",
      "dice[0.771462262 0.0870712623]\n",
      "dice[0.755802572 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 25, loss=5.676372528076172, loss_1=5.64855432510376, loss_2=5.610710620880127 (12.852392s)\n",
      "dice[0.811986923 0]\n",
      "dice[0.866988122 0]\n",
      "dice[0.864716172 0]\n",
      "dice[0.749446392 0]\n",
      "dice[0.761342764 0.087149322]\n",
      "dice[0.816750109 0.0698244125]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 26, loss=5.803001403808594, loss_1=10.59645938873291, loss_2=5.612414836883545 (12.630396s)\n",
      "dice[0.868594 0.0805060267]\n",
      "dice[0.861447632 0.083134383]\n",
      "dice[0.808790147 0.0669523627]\n",
      "dice[0.757133842 0.0937973559]\n",
      "dice[0.759708047 0.0909986]\n",
      "dice[0.81787914 0.0696567446]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 27, loss=1.0623953342437744, loss_1=0.6546141505241394, loss_2=0.6425464153289795 (17.807755s)\n",
      "dice[0.80912286 0.0664116293]\n",
      "dice[0.868082762 0.0828230679]\n",
      "dice[0.867232442 0.0815373063]\n",
      "dice[0.75119704 0]\n",
      "dice[0.82357204 0]\n",
      "dice[0.761277318 0.0930862501]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 28, loss=5.8572678565979, loss_1=5.64823579788208, loss_2=0.6463844180107117 (15.944943s)\n",
      "dice[0.865628242 0]\n",
      "dice[0.813699424 0]\n",
      "dice[0.866253853 0.0823908076]\n",
      "dice[0.756924152 0]\n",
      "dice[0.821715415 0.068934992]\n",
      "dice[0.762392938 0.0930727199]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 29, loss=5.815171241760254, loss_1=0.6384901404380798, loss_2=10.594362258911133 (15.699771s)\n",
      "dice[0.864107549 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.865195811 0]\n",
      "dice[0.809409618 0]\n",
      "dice[0.74603045 0]\n",
      "dice[0.760094345 0.0941660628]\n",
      "dice[0.823404 0.0686674416]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 30, loss=5.820061683654785, loss_1=5.578917026519775, loss_2=10.597192764282227 (12.820049s)\n",
      "dice[0.809777617 0]\n",
      "dice[0.869936109 0.0818677321]\n",
      "dice[0.856661 0.0866767466]\n",
      "dice[0.825407088 0.0684065]\n",
      "dice[0.753730476 0]\n",
      "dice[0.749255538 0.0950433761]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 31, loss=5.823639869689941, loss_1=0.6393111944198608, loss_2=5.620110034942627 (15.898072s)\n",
      "dice[0.87062186 0]\n",
      "dice[0.876215219 0.082265459]\n",
      "dice[0.806451499 0.0667684376]\n",
      "dice[0.740569115 0]\n",
      "dice[0.73221916 0]\n",
      "dice[0.830324 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 32, loss=5.850198268890381, loss_1=10.59720230102539, loss_2=5.655953407287598 (16.330065s)\n",
      "dice[0.873694241 0]\n",
      "dice[0.871383 0]\n",
      "dice[0.805048943 0.0667463765]\n",
      "dice[0.740012586 0]\n",
      "dice[0.753977239 0.0974068791]\n",
      "dice[0.828255713 0.0680708364]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 33, loss=1.0892882347106934, loss_1=5.57098913192749, loss_2=10.596572875976562 (13.844125s)\n",
      "dice[0.804243088 0.0655491129]\n",
      "dice[0.86489141 0.0845967904]\n",
      "dice[0.865735829 0.0842741057]\n",
      "dice[0.834062934 0]\n",
      "dice[0.729944468 0]\n",
      "dice[0.732593358 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 34, loss=5.870752334594727, loss_1=5.6418375968933105, loss_2=5.638794422149658 (12.824929s)\n",
      "dice[0.875844955 0]\n",
      "dice[0.885833502 0.0801470652]\n",
      "dice[0.804684222 0.0653506592]\n",
      "dice[0.751089 0.0973962]\n",
      "dice[0.726477802 0]\n",
      "dice[0.833203793 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 35, loss=5.874334335327148, loss_1=5.57061243057251, loss_2=5.675419807434082 (16.375730s)\n",
      "dice[0.878477156 0]\n",
      "dice[0.884730279 0.0784617]\n",
      "dice[0.888136804 0.0797239691]\n",
      "dice[0.726571679 0]\n",
      "dice[0.748615861 0.0976784527]\n",
      "dice[0.745560825 0.0972488523]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 36, loss=5.570154666900635, loss_1=0.6663663387298584, loss_2=5.679171085357666 (16.297861s)\n",
      "dice[0.879247606 0]\n",
      "dice[0.884053588 0]\n",
      "dice[0.866772652 0.082010895]\n",
      "dice[0.7312361 0]\n",
      "dice[0.747959673 0.0980855301]\n",
      "dice[0.739964724 0.0943326727]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 37, loss=5.5695929527282715, loss_1=0.643161416053772, loss_2=10.59617805480957 (15.707477s)\n",
      "dice[0.88128376 0]\n",
      "dice[0.884357572 0]\n",
      "dice[0.870287299 0.0791012347]\n",
      "dice[0.730581582 0]\n",
      "dice[0.748415172 0.0983400643]\n",
      "dice[0.722199798 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 38, loss=10.59626579284668, loss_1=5.691292762756348, loss_2=5.568679332733154 (14.124959s)\n",
      "dice[0.885693967 0]\n",
      "dice[0.871855199 0.078853786]\n",
      "dice[0.874236584 0.0824615061]\n",
      "dice[0.730038285 0]\n",
      "dice[0.722209215 0]\n",
      "dice[0.711791575 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 39, loss=10.596067428588867, loss_1=5.69356107711792, loss_2=5.6597771644592285 (14.079573s)\n",
      "dice[0.886898577 0]\n",
      "dice[0.869250238 0.0775093064]\n",
      "dice[0.869250238 0.0775093064]\n",
      "dice[0.730076969 0]\n",
      "dice[0.743259192 0.0971759]\n",
      "dice[0.743259192 0.0971759]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 40, loss=10.595756530761719, loss_1=0.6816530823707581, loss_2=0.6816530823707581 (14.064223s)\n",
      "dice[0.868105173 0]\n",
      "dice[0.882037759 0]\n",
      "dice[0.8735829 0.0784762576]\n",
      "dice[0.723540783 0]\n",
      "dice[0.722214937 0]\n",
      "dice[0.748032451 0.0978535637]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 41, loss=5.697249412536621, loss_1=5.569170951843262, loss_2=10.602088928222656 (12.788686s)\n",
      "dice[0.888684154 0]\n",
      "dice[0.865664363 0]\n",
      "dice[0.869431496 0.0771784037]\n",
      "dice[0.729716897 0]\n",
      "dice[0.738108754 0.0948798433]\n",
      "dice[0.743752539 0.0963333249]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 42, loss=10.595399856567383, loss_1=5.58189058303833, loss_2=0.6868934631347656 (14.077978s)\n",
      "dice[0.889863849 0]\n",
      "dice[0.875213921 0.0781514421]\n",
      "dice[0.870853245 0.0769821778]\n",
      "dice[0.729723692 0]\n",
      "dice[0.722023785 0]\n",
      "dice[0.744336605 0.0964149907]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 43, loss=5.700492858886719, loss_1=10.595102310180664, loss_2=0.6885213851928711 (15.917531s)\n",
      "dice[0.871341646 0]\n",
      "dice[0.871973753 0.0767649636]\n",
      "dice[0.876906395 0.0820380524]\n",
      "dice[0.72619313 0]\n",
      "dice[0.709155262 0]\n",
      "dice[0.744342625 0.0966202915]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 44, loss=0.6903969049453735, loss_1=5.663632869720459, loss_2=10.600616455078125 (13.711205s)\n",
      "dice[0.886145234 0]\n",
      "dice[0.878593 0.0780685768]\n",
      "dice[0.872456789 0.0765277669]\n",
      "dice[0.722792387 0]\n",
      "dice[0.747788131 0.0976042598]\n",
      "dice[0.744336188 0.0968572721]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 45, loss=5.568550109863281, loss_1=0.6926610469818115, loss_2=5.700383186340332 (15.911865s)\n",
      "dice[0.867411673 0]\n",
      "dice[0.867411673 0]\n",
      "dice[0.873871863 0.0762688741]\n",
      "dice[0.743827522 0.0978815183]\n",
      "dice[0.743827522 0.0978815183]\n",
      "dice[0.74509424 0.0974045843]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 46, loss=5.573841571807861, loss_1=0.6943157315254211, loss_2=5.573841571807861 (16.068534s)\n",
      "dice[0.875716805 0]\n",
      "dice[0.876573384 0]\n",
      "dice[0.869080961 0]\n",
      "dice[0.720175266 0]\n",
      "dice[0.726728797 0]\n",
      "dice[0.74418658 0.0981072634]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 47, loss=10.599174499511719, loss_1=10.60102653503418, loss_2=5.573051929473877 (14.529318s)\n",
      "dice[0.880109966 0]\n",
      "dice[0.870625079 0]\n",
      "dice[0.801944315 0.0647753403]\n",
      "dice[0.71444577 0]\n",
      "dice[0.847667336 0]\n",
      "dice[0.744425833 0.0984186903]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 48, loss=5.881597518920898, loss_1=5.572257995605469, loss_2=10.601360321044922 (16.178070s)\n",
      "dice[0.879796505 0]\n",
      "dice[0.884511471 0.076532051]\n",
      "dice[0.802004278 0.0642390475]\n",
      "dice[0.726874352 0]\n",
      "dice[0.722896159 0]\n",
      "dice[0.848061681 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 49, loss=5.891135215759277, loss_1=5.716701030731201, loss_2=10.598331451416016 (12.779784s)\n",
      "dice[0.881311655 0]\n",
      "dice[0.874503732 0]\n",
      "dice[0.880621433 0.0749000311]\n",
      "dice[0.727123082 0]\n",
      "dice[0.745558441 0.100752778]\n",
      "dice[0.747472048 0.0977362394]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 50, loss=0.7086008191108704, loss_1=10.597890853881836, loss_2=5.569796085357666 (13.681876s)\n",
      "dice[0.879050612 0]\n",
      "dice[0.800301254 0.0645155683]\n",
      "dice[0.883321881 0.0744645]\n",
      "dice[0.730697453 0.0934354588]\n",
      "dice[0.844541371 0.0671343282]\n",
      "dice[0.747804165 0.0978130698]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 51, loss=1.1407012939453125, loss_1=0.7133601903915405, loss_2=5.584977149963379 (17.518201s)\n",
      "dice[0.878186464 0]\n",
      "dice[0.877098441 0]\n",
      "dice[0.890851736 0.0752801225]\n",
      "dice[0.74679476 0.102639914]\n",
      "dice[0.722626388 0]\n",
      "dice[0.735220969 0.0946076512]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 52, loss=5.5680952072143555, loss_1=5.580537796020508, loss_2=5.730578422546387 (16.323151s)\n",
      "dice[0.881521761 0]\n",
      "dice[0.886686742 0.0763868093]\n",
      "dice[0.886686742 0.0763868093]\n",
      "dice[0.727683544 0]\n",
      "dice[0.724171758 0]\n",
      "dice[0.724171758 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 53, loss=10.597698211669922, loss_1=5.717584609985352, loss_2=5.717584609985352 (14.007800s)\n",
      "dice[0.861876547 0]\n",
      "dice[0.881453574 0.0773346499]\n",
      "dice[0.881453574 0.0773346499]\n",
      "dice[0.723681033 0]\n",
      "dice[0.742858768 0.0941177607]\n",
      "dice[0.723681033 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 54, loss=5.707812309265137, loss_1=5.707812309265137, loss_2=5.58393669128418 (12.887182s)\n",
      "dice[0.871704459 0]\n",
      "dice[0.869700909 0.0771820545]\n",
      "dice[0.877857804 0.0782265291]\n",
      "dice[0.724757671 0]\n",
      "dice[0.722340941 0]\n",
      "dice[0.747809768 0.0957453921]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 55, loss=10.600884437561035, loss_1=0.6870805025100708, loss_2=5.698915004730225 (14.056484s)\n",
      "dice[0.885940254 0]\n",
      "dice[0.865843296 0.0779879]\n",
      "dice[0.865843296 0.0779879]\n",
      "dice[0.726655 0]\n",
      "dice[0.746968806 0.0953103304]\n",
      "dice[0.746968806 0.0953103304]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 56, loss=0.6801037788391113, loss_1=10.596851348876953, loss_2=0.6801037788391113 (14.060817s)\n",
      "dice[0.87298435 0]\n",
      "dice[0.802456737 0]\n",
      "dice[0.881834328 0.078550905]\n",
      "dice[0.748194456 0.0953421891]\n",
      "dice[0.850047588 0.0665480122]\n",
      "dice[0.74916178 0.0949258581]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 57, loss=5.849996089935303, loss_1=5.576293468475342, loss_2=0.6703344583511353 (15.847987s)\n",
      "dice[0.869949 0]\n",
      "dice[0.854934216 0]\n",
      "dice[0.864238799 0.0841665193]\n",
      "dice[0.721214533 0]\n",
      "dice[0.747896135 0.0950494707]\n",
      "dice[0.709202766 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 58, loss=5.5779032707214355, loss_1=5.64827299118042, loss_2=10.605962753295898 (16.106017s)\n",
      "dice[0.877770483 0.0793118551]\n",
      "dice[0.886855185 0.0796685889]\n",
      "dice[0.886855185 0.0796685889]\n",
      "dice[0.711519122 0]\n",
      "dice[0.711519122 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.748231292 0.0940852836]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 59, loss=0.6658960580825806, loss_1=5.683830738067627, loss_2=5.683830738067627 (13.465711s)\n",
      "dice[0.86737144 0]\n",
      "dice[0.86737144 0]\n",
      "dice[0.804524541 0.0640036091]\n",
      "dice[0.732848108 0.0947576389]\n",
      "dice[0.852964163 0]\n",
      "dice[0.732848108 0.0947576389]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 60, loss=5.893561840057373, loss_1=5.583126544952393, loss_2=5.583126544952393 (12.899364s)\n",
      "dice[0.876808405 0]\n",
      "dice[0.805013299 0]\n",
      "dice[0.851224 0.084583208]\n",
      "dice[0.716966391 0]\n",
      "dice[0.850667179 0.0664108321]\n",
      "dice[0.737756312 0.0892504901]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 61, loss=5.851534843444824, loss_1=0.6476038098335266, loss_2=10.601556777954102 (15.720189s)\n",
      "dice[0.864775836 0]\n",
      "dice[0.87400794 0]\n",
      "dice[0.805051565 0.0639885217]\n",
      "dice[0.717511117 0]\n",
      "dice[0.732811928 0.0946166292]\n",
      "dice[0.853192925 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 62, loss=5.893648147583008, loss_1=10.602120399475098, loss_2=5.584193706512451 (12.717728s)\n",
      "dice[0.864456177 0]\n",
      "dice[0.841068864 0]\n",
      "dice[0.864456177 0]\n",
      "dice[0.73283112 0.0944778323]\n",
      "dice[0.73283112 0.0944778323]\n",
      "dice[0.747802615 0.0899237171]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 63, loss=5.584682464599609, loss_1=5.584682464599609, loss_2=5.605684280395508 (16.292944s)\n",
      "dice[0.864460349 0]\n",
      "dice[0.874112904 0.0799625665]\n",
      "dice[0.874112904 0.0799625665]\n",
      "dice[0.732990086 0.0944408849]\n",
      "dice[0.731765 0.0933385119]\n",
      "dice[0.731765 0.0933385119]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 64, loss=0.6666738390922546, loss_1=0.6666738390922546, loss_2=5.584753036499023 (13.985133s)\n",
      "dice[0.87398 0]\n",
      "dice[0.857633412 0.0853965878]\n",
      "dice[0.882209957 0.0802000239]\n",
      "dice[0.718465 0]\n",
      "dice[0.710218668 0]\n",
      "dice[0.699488461 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 65, loss=5.679852485656738, loss_1=10.601888656616211, loss_2=5.6426849365234375 (16.369002s)\n",
      "dice[0.873996079 0]\n",
      "dice[0.853431046 0.0861530453]\n",
      "dice[0.803676486 0.0644904077]\n",
      "dice[0.718456209 0]\n",
      "dice[0.708206 0]\n",
      "dice[0.850624144 0.0667166114]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 66, loss=1.1458019018173218, loss_1=5.635986804962158, loss_2=10.601886749267578 (13.922302s)\n",
      "dice[0.864520848 0]\n",
      "dice[0.874007285 0]\n",
      "dice[0.802469671 0]\n",
      "dice[0.718414187 0]\n",
      "dice[0.852686703 0.0665189549]\n",
      "dice[0.733378649 0.0944266617]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 67, loss=5.849826335906982, loss_1=10.60189437866211, loss_2=5.584683895111084 (16.113150s)\n",
      "dice[0.874080181 0]\n",
      "dice[0.874253094 0.079929024]\n",
      "dice[0.88262552 0.0801539645]\n",
      "dice[0.718450427 0]\n",
      "dice[0.710546732 0]\n",
      "dice[0.732047379 0.0932912827]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 68, loss=5.6801347732543945, loss_1=10.60186767578125, loss_2=0.6670825481414795 (15.875160s)\n",
      "dice[0.845436454 0]\n",
      "dice[0.856311321 0.0856623724]\n",
      "dice[0.803118348 0.0645375922]\n",
      "dice[0.702940345 0]\n",
      "dice[0.707412541 0]\n",
      "dice[0.855703115 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 69, loss=5.883555889129639, loss_1=10.612905502319336, loss_2=5.639045238494873 (12.742781s)\n",
      "dice[0.864736676 0]\n",
      "dice[0.864736676 0]\n",
      "dice[0.875219524 0.079848744]\n",
      "dice[0.733659148 0.0944002867]\n",
      "dice[0.733659148 0.0944002867]\n",
      "dice[0.732446492 0.093273513]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 70, loss=5.584640026092529, loss_1=5.584640026092529, loss_2=0.6676325798034668 (15.826755s)\n",
      "dice[0.874331713 0]\n",
      "dice[0.875303447 0.079793863]\n",
      "dice[0.806471586 0.0645002276]\n",
      "dice[0.718088627 0]\n",
      "dice[0.855905235 0]\n",
      "dice[0.732504487 0.0932802334]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 71, loss=5.883338928222656, loss_1=0.6681402921676636, loss_2=10.60189437866211 (15.830135s)\n",
      "dice[0.87441355 0]\n",
      "dice[0.883978844 0.0799590796]\n",
      "dice[0.883978844 0.0799590796]\n",
      "dice[0.717714906 0]\n",
      "dice[0.711237431 0]\n",
      "dice[0.711237431 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 72, loss=5.681615829467773, loss_1=10.601967811584473, loss_2=5.681615829467773 (12.777034s)\n",
      "dice[0.864503443 0]\n",
      "dice[0.864503443 0]\n",
      "dice[0.875712693 0.0796602815]\n",
      "dice[0.734227777 0.0944856927]\n",
      "dice[0.734227777 0.0944856927]\n",
      "dice[0.732892334 0.0933836699]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 73, loss=5.5842976570129395, loss_1=0.6689577102661133, loss_2=5.5842976570129395 (15.993685s)\n",
      "dice[0.807682931 0]\n",
      "dice[0.875145793 0]\n",
      "dice[0.850195944 0.0846464485]\n",
      "dice[0.858038902 0]\n",
      "dice[0.71730566 0]\n",
      "dice[0.737670839 0.0887276381]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 74, loss=10.583569526672363, loss_1=10.601886749267578, loss_2=0.6503891944885254 (14.484855s)\n",
      "dice[0.864869714 0]\n",
      "dice[0.807822049 0]\n",
      "dice[0.876067579 0.0795359239]\n",
      "dice[0.858441472 0]\n",
      "dice[0.734337747 0.0944865942]\n",
      "dice[0.733084381 0.093384549]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 75, loss=10.583434104919434, loss_1=5.584176063537598, loss_2=0.6701174974441528 (14.220133s)\n",
      "dice[0.842949 0]\n",
      "dice[0.87551564 0]\n",
      "dice[0.806634784 0.0655124336]\n",
      "dice[0.715599179 0]\n",
      "dice[0.731113553 0.0897889286]\n",
      "dice[0.85227257 0.0666442662]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 76, loss=1.1277334690093994, loss_1=5.610103607177734, loss_2=10.602221488952637 (13.745767s)\n",
      "dice[0.876585066 0]\n",
      "dice[0.877128422 0.0793812349]\n",
      "dice[0.877128422 0.0793812349]\n",
      "dice[0.715569079 0]\n",
      "dice[0.733751 0.0934000686]\n",
      "dice[0.733751 0.0934000686]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 77, loss=0.6712579727172852, loss_1=10.601961135864258, loss_2=0.6712579727172852 (17.529378s)\n",
      "dice[0.848676682 0]\n",
      "dice[0.808723927 0]\n",
      "dice[0.877429247 0.0793143511]\n",
      "dice[0.704472601 0]\n",
      "dice[0.859031796 0]\n",
      "dice[0.733967543 0.093404144]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 78, loss=10.583061218261719, loss_1=10.611713409423828, loss_2=0.6718215346336365 (14.647354s)\n",
      "dice[0.851212442 0]\n",
      "dice[0.878454447 0.0792292655]\n",
      "dice[0.878454447 0.0792292655]\n",
      "dice[0.708118141 0]\n",
      "dice[0.734388053 0.0934107453]\n",
      "dice[0.734388053 0.0934107453]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 79, loss=0.6723397970199585, loss_1=0.6723397970199585, loss_2=10.610167503356934 (14.034918s)\n",
      "dice[0.877908468 0]\n",
      "dice[0.867839336 0.0793733224]\n",
      "dice[0.867839336 0.0793733224]\n",
      "dice[0.71534729 0]\n",
      "dice[0.708951414 0]\n",
      "dice[0.708951414 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 80, loss=10.601686477661133, loss_1=5.692323684692383, loss_2=5.692323684692383 (14.148679s)\n",
      "dice[0.867630601 0]\n",
      "dice[0.878884077 0.0790729597]\n",
      "dice[0.80859524 0.0648486689]\n",
      "dice[0.858460367 0]\n",
      "dice[0.734988451 0.0945723951]\n",
      "dice[0.734748125 0.0934485272]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 81, loss=5.8759284019470215, loss_1=0.6736773252487183, loss_2=5.583066940307617 (16.118476s)\n",
      "dice[0.845164537 0]\n",
      "dice[0.878181756 0.0789793]\n",
      "dice[0.808899403 0.0648378059]\n",
      "dice[0.73229295 0.0898346826]\n",
      "dice[0.858622193 0]\n",
      "dice[0.735182822 0.0934566483]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 82, loss=5.876004695892334, loss_1=0.6747211217880249, loss_2=5.609010219573975 (16.033745s)\n",
      "dice[0.878427088 0]\n",
      "dice[0.878427088 0]\n",
      "dice[0.861279428 0.0846925527]\n",
      "dice[0.715344787 0]\n",
      "dice[0.715344787 0]\n",
      "dice[0.704385757 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 83, loss=10.601556777954102, loss_1=5.645989894866943, loss_2=10.601556777954102 (14.683115s)\n",
      "dice[0.868974686 0]\n",
      "dice[0.869549453 0.0790377408]\n",
      "dice[0.808690906 0.0654150173]\n",
      "dice[0.710280955 0]\n",
      "dice[0.735646248 0.094637692]\n",
      "dice[0.854959488 0.0665934235]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 84, loss=1.129115343093872, loss_1=5.695137023925781, loss_2=5.582374095916748 (13.772004s)\n",
      "dice[0.81131959 0]\n",
      "dice[0.869297087 0]\n",
      "dice[0.855665505 0]\n",
      "dice[0.86119628 0]\n",
      "dice[0.704975367 0]\n",
      "dice[0.735795379 0.094649978]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 85, loss=10.581871032714844, loss_1=5.582220077514648, loss_2=10.609840393066406 (14.510004s)\n",
      "dice[0.870744109 0.0788745135]\n",
      "dice[0.809208691 0.0654372275]\n",
      "dice[0.870744109 0.0788745135]\n",
      "dice[0.710975885 0]\n",
      "dice[0.710975885 0]\n",
      "dice[0.855326891 0.0665415376]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 86, loss=1.1293847560882568, loss_1=5.696423053741455, loss_2=5.696423053741455 (13.672102s)\n",
      "dice[0.870681763 0]\n",
      "dice[0.870681763 0]\n",
      "dice[0.880007803 0.078564696]\n",
      "dice[0.736204684 0.09467794]\n",
      "dice[0.736204684 0.09467794]\n",
      "dice[0.736215115 0.0935698748]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 87, loss=5.581689834594727, loss_1=5.581689834594727, loss_2=0.6781153082847595 (16.089609s)\n",
      "dice[0.870820224 0]\n",
      "dice[0.855427 0]\n",
      "dice[0.809992433 0.0647693574]\n",
      "dice[0.710431337 0]\n",
      "dice[0.736297 0.0946938694]\n",
      "dice[0.856848955 0.0664874688]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 88, loss=1.141547441482544, loss_1=5.5815863609313965, loss_2=10.608535766601562 (13.820573s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.881517708 0]\n",
      "dice[0.872064829 0.0786053836]\n",
      "dice[0.88135016 0.0783829316]\n",
      "dice[0.716784179 0]\n",
      "dice[0.711818218 0]\n",
      "dice[0.736940742 0.0940498784]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 89, loss=10.600423812866211, loss_1=5.69881010055542, loss_2=0.6779944896697998 (14.179404s)\n",
      "dice[0.881659448 0.0782974213]\n",
      "dice[0.811821103 0.0647856519]\n",
      "dice[0.881659448 0.0782974213]\n",
      "dice[0.861603081 0]\n",
      "dice[0.737132251 0.0940599591]\n",
      "dice[0.737132251 0.0940599591]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 90, loss=5.875460147857666, loss_1=0.6787842512130737, loss_2=0.6787842512130737 (15.446760s)\n",
      "dice[0.847771466 0]\n",
      "dice[0.865043163 0.0840013102]\n",
      "dice[0.812057495 0.0647588074]\n",
      "dice[0.703919411 0]\n",
      "dice[0.861782968 0]\n",
      "dice[0.738882 0.0914072767]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 91, loss=5.87583589553833, loss_1=5.598943710327148, loss_2=5.6507487297058105 (16.431784s)\n",
      "dice[0.814258039 0]\n",
      "dice[0.88288641 0.0781317279]\n",
      "dice[0.88288641 0.0781317279]\n",
      "dice[0.864241183 0]\n",
      "dice[0.737841964 0.0940870717]\n",
      "dice[0.737841964 0.0940870717]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 92, loss=10.580375671386719, loss_1=0.6800591945648193, loss_2=0.6800591945648193 (14.118402s)\n",
      "dice[0.883092403 0]\n",
      "dice[0.873498261 0.0782542452]\n",
      "dice[0.882984579 0.0780358091]\n",
      "dice[0.717618644 0]\n",
      "dice[0.713001 0]\n",
      "dice[0.737981558 0.0941075906]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 93, loss=5.702031135559082, loss_1=0.6810091137886047, loss_2=10.599822044372559 (15.849816s)\n",
      "dice[0.855712771 0]\n",
      "dice[0.883222103 0]\n",
      "dice[0.87382108 0]\n",
      "dice[0.708356261 0]\n",
      "dice[0.717786372 0]\n",
      "dice[0.736701 0.0947983637]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 94, loss=10.599747657775879, loss_1=5.580434322357178, loss_2=10.608983039855957 (14.640316s)\n",
      "dice[0.874907553 0]\n",
      "dice[0.884141445 0]\n",
      "dice[0.884141445 0]\n",
      "dice[0.718227148 0]\n",
      "dice[0.718227148 0]\n",
      "dice[0.737082064 0.0948068127]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 95, loss=10.599407196044922, loss_1=10.599407196044922, loss_2=5.580042839050293 (14.484729s)\n",
      "dice[0.875020385 0]\n",
      "dice[0.860939145 0.0828467235]\n",
      "dice[0.856888771 0.0835514888]\n",
      "dice[0.737169206 0.0948218331]\n",
      "dice[0.724933445 0.089030385]\n",
      "dice[0.743540168 0.0894934312]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 96, loss=5.57995080947876, loss_1=0.6518669128417969, loss_2=0.6642043590545654 (15.230859s)\n",
      "dice[0.875343859 0]\n",
      "dice[0.864735126 0.0777093396]\n",
      "dice[0.811926723 0.0646330789]\n",
      "dice[0.737337828 0.0948317349]\n",
      "dice[0.864623308 0]\n",
      "dice[0.73570013 0.0941822529]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 97, loss=5.877408981323242, loss_1=5.579799652099609, loss_2=0.6895982027053833 (16.074745s)\n",
      "dice[0.865765512 0.0776315928]\n",
      "dice[0.869987428 0.0831702054]\n",
      "dice[0.812241435 0.0648983493]\n",
      "dice[0.699815333 0]\n",
      "dice[0.865022123 0]\n",
      "dice[0.736153126 0.0941985324]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 98, loss=5.872490882873535, loss_1=5.657567024230957, loss_2=0.6900635361671448 (16.258088s)\n",
      "dice[0.886743724 0]\n",
      "dice[0.810271323 0]\n",
      "dice[0.865872681 0.0775469393]\n",
      "dice[0.719593763 0]\n",
      "dice[0.83975929 0.0659956187]\n",
      "dice[0.736283779 0.0942091569]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 99, loss=5.860067844390869, loss_1=10.59841537475586, loss_2=0.690940260887146 (15.738526s)\n",
      "dice[0.886854887 0]\n",
      "dice[0.886854887 0]\n",
      "dice[0.876349032 0.0780597553]\n",
      "dice[0.719760418 0]\n",
      "dice[0.719760418 0]\n",
      "dice[0.714603722 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 100, loss=10.598345756530762, loss_1=5.703090667724609, loss_2=10.598345756530762 (14.615347s)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m iter 100 saved: /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models/model.ckpt\n",
      "dice[0.876976073 0]\n",
      "dice[0.868220627 0.0836561322]\n",
      "dice[0.813153148 0.0648233443]\n",
      "dice[0.702555954 0]\n",
      "dice[0.738169432 0.0949846804]\n",
      "dice[0.86675781 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 101, loss=5.873165607452393, loss_1=5.578755855560303, loss_2=5.653172016143799 (12.960044s)\n",
      "dice[0.877087593 0]\n",
      "dice[0.877675056 0.0779181346]\n",
      "dice[0.867336452 0.0773171186]\n",
      "dice[0.715241 0]\n",
      "dice[0.738270402 0.0949937701]\n",
      "dice[0.737223446 0.0943552628]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 102, loss=5.704193592071533, loss_1=0.6925359964370728, loss_2=5.578678131103516 (15.857924s)\n",
      "dice[0.853647888 0]\n",
      "dice[0.810089231 0.0650372058]\n",
      "dice[0.877778411 0.0778422]\n",
      "dice[0.715383947 0]\n",
      "dice[0.741817 0.091616191]\n",
      "dice[0.862615585 0.0660757348]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 103, loss=1.1423587799072266, loss_1=5.59580135345459, loss_2=5.704990863800049 (13.564679s)\n",
      "dice[0.810725629 0]\n",
      "dice[0.887705803 0]\n",
      "dice[0.878267348 0]\n",
      "dice[0.846924663 0]\n",
      "dice[0.720679343 0]\n",
      "dice[0.738858342 0.0950356349]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 104, loss=10.585587501525879, loss_1=10.59790325164795, loss_2=5.578120708465576 (14.595531s)\n",
      "dice[0.810065508 0]\n",
      "dice[0.861226797 0]\n",
      "dice[0.87937212 0]\n",
      "dice[0.711057305 0]\n",
      "dice[0.842977941 0.0657806695]\n",
      "dice[0.739281774 0.0950463265]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 105, loss=5.863034248352051, loss_1=10.606928825378418, loss_2=5.577709674835205 (16.086256s)\n",
      "dice[0.870011747 0]\n",
      "dice[0.879481 0]\n",
      "dice[0.808805466 0.0650002286]\n",
      "dice[0.717817962 0]\n",
      "dice[0.739390314 0.0950590372]\n",
      "dice[0.841856718 0.0659700483]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 106, loss=1.1503472328186035, loss_1=5.577620506286621, loss_2=10.603042602539062 (13.900414s)\n",
      "dice[0.870104432 0]\n",
      "dice[0.86881882 0.0769696534]\n",
      "dice[0.86881882 0.0769696534]\n",
      "dice[0.7179721 0]\n",
      "dice[0.738440752 0.0944082513]\n",
      "dice[0.738440752 0.0944082513]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 107, loss=10.602980613708496, loss_1=0.695756733417511, loss_2=0.695756733417511 (14.117676s)\n",
      "dice[0.87936455 0.0775351226]\n",
      "dice[0.87936455 0.0775351226]\n",
      "dice[0.873492718 0.0826770812]\n",
      "dice[0.716628492 0]\n",
      "dice[0.716628492 0]\n",
      "dice[0.705864549 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 108, loss=5.707785606384277, loss_1=5.659512519836426, loss_2=5.707785606384277 (12.992616s)\n",
      "dice[0.871295333 0]\n",
      "dice[0.880715907 0]\n",
      "dice[0.879448891 0.077493459]\n",
      "dice[0.718667626 0]\n",
      "dice[0.716760099 0]\n",
      "dice[0.739987 0.0951412171]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 109, loss=5.708210468292236, loss_1=10.602508544921875, loss_2=5.576940536499023 (16.198121s)\n",
      "dice[0.867151618 0]\n",
      "dice[0.870997608 0.0830534697]\n",
      "dice[0.870166659 0.0768232569]\n",
      "dice[0.711361289 0]\n",
      "dice[0.703698337 0]\n",
      "dice[0.739238381 0.0944451839]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 110, loss=0.696835994720459, loss_1=5.6573591232299805, loss_2=10.605371475219727 (13.930251s)\n",
      "dice[0.812462926 0]\n",
      "dice[0.871614575 0]\n",
      "dice[0.870281756 0.0767904595]\n",
      "dice[0.849805415 0]\n",
      "dice[0.719352305 0]\n",
      "dice[0.739343047 0.0944820717]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 111, loss=10.584432601928711, loss_1=0.6970582604408264, loss_2=10.602258682250977 (14.493582s)\n",
      "dice[0.865735769 0]\n",
      "dice[0.870509088 0.0767671242]\n",
      "dice[0.880444646 0.077420786]\n",
      "dice[0.715999424 0]\n",
      "dice[0.716217697 0]\n",
      "dice[0.739521086 0.0944818929]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 112, loss=5.708934783935547, loss_1=10.60456657409668, loss_2=0.6972342729568481 (15.779459s)\n",
      "dice[0.865623772 0.0820184946]\n",
      "dice[0.87063235 0.0767369196]\n",
      "dice[0.811397 0.0643743202]\n",
      "dice[0.732958317 0.0906818062]\n",
      "dice[0.739817202 0.0944880843]\n",
      "dice[0.844977856 0.0657743737]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 113, loss=1.1635147333145142, loss_1=0.6974693536758423, loss_2=0.6597201824188232 (17.868697s)\n",
      "dice[0.815048695 0]\n",
      "dice[0.857284665 0]\n",
      "dice[0.880772948 0.0773655623]\n",
      "dice[0.851347685 0]\n",
      "dice[0.716595173 0]\n",
      "dice[0.74380672 0.0917884633]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 114, loss=10.58340072631836, loss_1=5.593637466430664, loss_2=5.7093963623046875 (14.110282s)\n",
      "dice[0.882098138 0.0772231221]\n",
      "dice[0.87191546 0.0765515342]\n",
      "dice[0.882098138 0.0772231221]\n",
      "dice[0.7172364 0]\n",
      "dice[0.7172364 0]\n",
      "dice[0.740314424 0.0945578068]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 115, loss=0.6990271806716919, loss_1=5.710557460784912, loss_2=5.710557460784912 (13.689111s)\n",
      "dice[0.873379469 0]\n",
      "dice[0.863100469 0]\n",
      "dice[0.872265041 0.0764030442]\n",
      "dice[0.720685422 0]\n",
      "dice[0.718285143 0]\n",
      "dice[0.740617335 0.0945968777]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 116, loss=0.7005319595336914, loss_1=10.604654312133789, loss_2=10.601484298706055 (13.899719s)\n",
      "dice[0.874429703 0]\n",
      "dice[0.883449912 0.0769298226]\n",
      "dice[0.811943 0.0641616583]\n",
      "dice[0.721205652 0]\n",
      "dice[0.718058 0]\n",
      "dice[0.850550473 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 117, loss=5.889432430267334, loss_1=10.601091384887695, loss_2=5.713449001312256 (12.860721s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.885174751 0]\n",
      "dice[0.873512387 0.0761185661]\n",
      "dice[0.873512387 0.0761185661]\n",
      "dice[0.742323577 0.0958754793]\n",
      "dice[0.741260111 0.0951508582]\n",
      "dice[0.741260111 0.0951508582]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 118, loss=0.7019487619400024, loss_1=5.573409557342529, loss_2=0.7019487619400024 (17.440697s)\n",
      "dice[0.876008 0]\n",
      "dice[0.885519505 0]\n",
      "dice[0.871956944 0]\n",
      "dice[0.722080231 0]\n",
      "dice[0.714006245 0]\n",
      "dice[0.742551506 0.095896259]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 119, loss=5.57321834564209, loss_1=10.603509902954102, loss_2=10.600478172302246 (12.392010s)\n",
      "dice[0.815504611 0]\n",
      "dice[0.885852 0]\n",
      "dice[0.886033475 0.0765683874]\n",
      "dice[0.85256052 0]\n",
      "dice[0.74277091 0.0959502459]\n",
      "dice[0.719316244 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 120, loss=10.58298397064209, loss_1=5.716780662536621, loss_2=5.572956562042236 (14.159405s)\n",
      "dice[0.877238154 0]\n",
      "dice[0.873114824 0.0807839856]\n",
      "dice[0.867148876 0.0817040652]\n",
      "dice[0.722569525 0]\n",
      "dice[0.737168431 0.0916097686]\n",
      "dice[0.730468869 0.0900686756]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 121, loss=10.600048065185547, loss_1=0.6568764448165894, loss_2=0.6733624935150146 (14.161647s)\n",
      "dice[0.867466092 0]\n",
      "dice[0.812886298 0.0645832196]\n",
      "dice[0.887270749 0.0763543]\n",
      "dice[0.739295959 0.0959976912]\n",
      "dice[0.851949573 0]\n",
      "dice[0.720051587 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 122, loss=5.881232261657715, loss_1=5.718860626220703, loss_2=5.578314781188965 (16.369488s)\n",
      "dice[0.868607819 0]\n",
      "dice[0.868607819 0]\n",
      "dice[0.811756611 0.0646297038]\n",
      "dice[0.739726901 0.0960170776]\n",
      "dice[0.739726901 0.0960170776]\n",
      "dice[0.847266495 0.0655476153]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 123, loss=1.1622060537338257, loss_1=5.577878475189209, loss_2=5.577878475189209 (13.910060s)\n",
      "dice[0.87877214 0]\n",
      "dice[0.887713373 0.0761663616]\n",
      "dice[0.811888695 0.0646570772]\n",
      "dice[0.723494291 0]\n",
      "dice[0.720209837 0]\n",
      "dice[0.847342074 0.0655524507]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 124, loss=1.161578893661499, loss_1=10.599432945251465, loss_2=5.720987796783447 (13.843324s)\n",
      "dice[0.871723294 0]\n",
      "dice[0.879997253 0]\n",
      "dice[0.888032794 0.076062791]\n",
      "dice[0.716310799 0]\n",
      "dice[0.724027634 0]\n",
      "dice[0.720464885 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 125, loss=5.722107410430908, loss_1=10.602991104125977, loss_2=10.598993301391602 (12.446270s)\n",
      "dice[0.870159626 0]\n",
      "dice[0.878924787 0.0752621368]\n",
      "dice[0.878924787 0.0752621368]\n",
      "dice[0.7406165 0.0962028727]\n",
      "dice[0.74316287 0.0955781564]\n",
      "dice[0.74316287 0.0955781564]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 126, loss=5.576859951019287, loss_1=0.7096467018127441, loss_2=0.7096467018127441 (15.510087s)\n",
      "dice[0.881207 0]\n",
      "dice[0.815422714 0]\n",
      "dice[0.879242897 0.0751649067]\n",
      "dice[0.724602878 0]\n",
      "dice[0.853306532 0]\n",
      "dice[0.743387103 0.0956070274]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 127, loss=10.582818031311035, loss_1=0.7106695175170898, loss_2=10.59854793548584 (14.692402s)\n",
      "dice[0.871380627 0]\n",
      "dice[0.812121511 0.0648515821]\n",
      "dice[0.872983396 0.080718644]\n",
      "dice[0.741209269 0.0962392315]\n",
      "dice[0.847955465 0.0655415133]\n",
      "dice[0.73364675 0.090517275]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 128, loss=1.158082127571106, loss_1=5.576328754425049, loss_2=0.6709567308425903 (17.528182s)\n",
      "dice[0.815515339 0]\n",
      "dice[0.88164109 0]\n",
      "dice[0.886879861 0.0804190934]\n",
      "dice[0.725015163 0]\n",
      "dice[0.854428947 0]\n",
      "dice[0.699597538 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 129, loss=10.582513809204102, loss_1=5.679129123687744, loss_2=10.598335266113281 (14.711413s)\n",
      "dice[0.815443635 0]\n",
      "dice[0.874403358 0.0805254802]\n",
      "dice[0.880572259 0.0748766735]\n",
      "dice[0.854533076 0]\n",
      "dice[0.734226108 0.0906175]\n",
      "dice[0.744215369 0.0957711115]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 130, loss=10.582505226135254, loss_1=0.7134073972702026, loss_2=0.6718789339065552 (14.226026s)\n",
      "dice[0.873171926 0]\n",
      "dice[0.880888641 0.0747790113]\n",
      "dice[0.89188236 0.0755049735]\n",
      "dice[0.722494721 0]\n",
      "dice[0.742092729 0.0963158086]\n",
      "dice[0.74446857 0.0957889631]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 131, loss=5.575498104095459, loss_1=5.7275309562683105, loss_2=0.714476466178894 (15.939479s)\n",
      "dice[0.883108854 0]\n",
      "dice[0.883108854 0]\n",
      "dice[0.892009914 0.0754080638]\n",
      "dice[0.724911571 0]\n",
      "dice[0.724911571 0]\n",
      "dice[0.722684801 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 132, loss=10.597994804382324, loss_1=10.597994804382324, loss_2=5.728665351867676 (14.567305s)\n",
      "dice[0.882763445 0.0810165629]\n",
      "dice[0.884279251 0.0807365552]\n",
      "dice[0.81306237 0.0648669526]\n",
      "dice[0.701660633 0]\n",
      "dice[0.705934465 0]\n",
      "dice[0.84875834 0.0654911]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 133, loss=1.1582540273666382, loss_1=5.676101207733154, loss_2=5.672664165496826 (13.620469s)\n",
      "dice[0.81348139 0.0643658116]\n",
      "dice[0.893186271 0.0752254799]\n",
      "dice[0.893186271 0.0752254799]\n",
      "dice[0.718577147 0]\n",
      "dice[0.85473 0]\n",
      "dice[0.718577147 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 134, loss=5.884305000305176, loss_1=5.731696605682373, loss_2=5.731696605682373 (16.444180s)\n",
      "dice[0.87582 0]\n",
      "dice[0.815397799 0]\n",
      "dice[0.893494606 0.0751253664]\n",
      "dice[0.716703415 0]\n",
      "dice[0.85038656 0.0652068779]\n",
      "dice[0.718803883 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 135, loss=5.869892597198486, loss_1=10.601869583129883, loss_2=5.732831001281738 (16.335269s)\n",
      "dice[0.885792553 0]\n",
      "dice[0.885792553 0]\n",
      "dice[0.884430826 0.0742946342]\n",
      "dice[0.72635746 0]\n",
      "dice[0.72635746 0]\n",
      "dice[0.746103346 0.0958891883]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 136, loss=0.7192366123199463, loss_1=10.596962928771973, loss_2=10.596962928771973 (13.778145s)\n",
      "dice[0.876007259 0]\n",
      "dice[0.894959629 0.0749388188]\n",
      "dice[0.813287079 0.0646106154]\n",
      "dice[0.743347764 0.0968502611]\n",
      "dice[0.855307519 0]\n",
      "dice[0.71950233 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 137, loss=5.879800796508789, loss_1=5.734665870666504, loss_2=5.573429107666016 (12.839310s)\n",
      "dice[0.886219323 0]\n",
      "dice[0.877175033 0]\n",
      "dice[0.886219323 0]\n",
      "dice[0.726714849 0]\n",
      "dice[0.726714849 0]\n",
      "dice[0.743804574 0.0968620107]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 138, loss=5.573001384735107, loss_1=10.596766471862793, loss_2=10.596766471862793 (12.247623s)\n",
      "dice[0.885145426 0.0740274042]\n",
      "dice[0.885145426 0.0740274042]\n",
      "dice[0.887377918 0.0801437646]\n",
      "dice[0.702815294 0]\n",
      "dice[0.746644497 0.0959311947]\n",
      "dice[0.746644497 0.0959311947]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 139, loss=0.7223455905914307, loss_1=5.680983066558838, loss_2=0.7223455905914307 (17.776816s)\n",
      "dice[0.817477286 0]\n",
      "dice[0.885992646 0.0739268064]\n",
      "dice[0.870631039 0.0796035901]\n",
      "dice[0.857578576 0]\n",
      "dice[0.706055284 0]\n",
      "dice[0.747076571 0.0959436]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 140, loss=10.581235885620117, loss_1=0.7233315110206604, loss_2=5.6899309158325195 (14.131013s)\n",
      "dice[0.816664696 0]\n",
      "dice[0.887492299 0]\n",
      "dice[0.8862589 0.0738386139]\n",
      "dice[0.72750175 0]\n",
      "dice[0.852965951 0.0892010927]\n",
      "dice[0.747263908 0.0964331701]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 141, loss=5.589446067810059, loss_1=0.723336398601532, loss_2=10.596251487731934 (15.826348s)\n",
      "dice[0.878791749 0]\n",
      "dice[0.896658361 0.0744723827]\n",
      "dice[0.896658361 0.0744723827]\n",
      "dice[0.744616449 0.0969267786]\n",
      "dice[0.720554411 0]\n",
      "dice[0.720554411 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 142, loss=5.572277545928955, loss_1=5.739993572235107, loss_2=5.739993572235107 (16.465199s)\n",
      "dice[0.878902853 0]\n",
      "dice[0.878902853 0]\n",
      "dice[0.89752686 0.0743817613]\n",
      "dice[0.720970035 0]\n",
      "dice[0.744654179 0.0969369709]\n",
      "dice[0.744654179 0.0969369709]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 143, loss=5.740853786468506, loss_1=5.5722222328186035, loss_2=5.5722222328186035 (16.267788s)\n",
      "dice[0.889849 0]\n",
      "dice[0.885266 0.0785109699]\n",
      "dice[0.814798176 0.067526]\n",
      "dice[0.728420138 0]\n",
      "dice[0.730442226 0.0906798244]\n",
      "dice[0.853487551 0.0891873389]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 144, loss=0.8366189002990723, loss_1=10.59543228149414, loss_2=0.690936267375946 (14.179709s)\n",
      "dice[0.816994369 0]\n",
      "dice[0.888878822 0.0734685138]\n",
      "dice[0.898951888 0.0741964728]\n",
      "dice[0.721654415 0]\n",
      "dice[0.855219305 0.0889514387]\n",
      "dice[0.748554528 0.0964954868]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 145, loss=5.590226650238037, loss_1=0.7272010445594788, loss_2=5.742754936218262 (15.980582s)\n",
      "dice[0.890251 0]\n",
      "dice[0.815159857 0.0683651343]\n",
      "dice[0.899250448 0.0740952939]\n",
      "dice[0.728795826 0]\n",
      "dice[0.860089719 0]\n",
      "dice[0.721858323 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 146, loss=5.814287185668945, loss_1=5.743962287902832, loss_2=10.595237731933594 (16.292826s)\n",
      "dice[0.873524427 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.875244319 0]\n",
      "dice[0.889103711 0.0732727647]\n",
      "dice[0.740775406 0.0933410227]\n",
      "dice[0.739141464 0.0918637738]\n",
      "dice[0.748797834 0.0965215191]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 147, loss=0.7296873331069946, loss_1=5.5841755867004395, loss_2=5.589987277984619 (13.414277s)\n",
      "dice[0.819447875 0]\n",
      "dice[0.900422454 0.0739115477]\n",
      "dice[0.900422454 0.0739115477]\n",
      "dice[0.862644196 0]\n",
      "dice[0.722213447 0]\n",
      "dice[0.722213447 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 148, loss=10.579477310180664, loss_1=5.7460150718688965, loss_2=5.7460150718688965 (14.299671s)\n",
      "dice[0.818296313 0.0684363544]\n",
      "dice[0.900529802 0.073819533]\n",
      "dice[0.890274704 0.0731060207]\n",
      "dice[0.722328365 0]\n",
      "dice[0.86188978 0]\n",
      "dice[0.745717943 0.0965764]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 149, loss=5.811910629272461, loss_1=5.747184753417969, loss_2=0.7323330640792847 (15.961536s)\n",
      "dice[0.892892182 0]\n",
      "dice[0.900832236 0.0737195686]\n",
      "dice[0.890577912 0.0730090141]\n",
      "dice[0.730104327 0]\n",
      "dice[0.722508729 0]\n",
      "dice[0.745905936 0.0965892375]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 150, loss=5.748400688171387, loss_1=0.7335160970687866, loss_2=10.59425163269043 (15.930279s)\n",
      "dice[0.883133054 0]\n",
      "dice[0.885384798 0]\n",
      "dice[0.878874302 0]\n",
      "dice[0.714303732 0]\n",
      "dice[0.745541394 0.0971771404]\n",
      "dice[0.735512614 0.0928038433]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 151, loss=5.570529460906982, loss_1=5.586148738861084, loss_2=10.600078582763672 (16.228242s)\n",
      "dice[0.883433461 0]\n",
      "dice[0.817026377 0.0693765208]\n",
      "dice[0.891994357 0.0728259]\n",
      "dice[0.745671511 0.0971887261]\n",
      "dice[0.859181225 0.0882452801]\n",
      "dice[0.746564209 0.0966178253]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 152, loss=0.8105353116989136, loss_1=5.570402145385742, loss_2=0.7354670763015747 (17.378747s)\n",
      "dice[0.889361382 0]\n",
      "dice[0.884300888 0]\n",
      "dice[0.817241788 0.0688580498]\n",
      "dice[0.71738714 0]\n",
      "dice[0.746097386 0.0972101614]\n",
      "dice[0.864632189 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 153, loss=5.80477237701416, loss_1=5.570044040679932, loss_2=10.598312377929688 (12.891972s)\n",
      "dice[0.884606 0]\n",
      "dice[0.820568442 0]\n",
      "dice[0.880046964 0.0785640702]\n",
      "dice[0.704387367 0]\n",
      "dice[0.746314406 0.0972256958]\n",
      "dice[0.861707687 0.0878383815]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 154, loss=5.594447612762451, loss_1=5.699125289916992, loss_2=5.569887638092041 (12.905275s)\n",
      "dice[0.885808289 0]\n",
      "dice[0.893282354 0.0725680962]\n",
      "dice[0.818438232 0.0690466]\n",
      "dice[0.865214765 0]\n",
      "dice[0.746781886 0.0972417891]\n",
      "dice[0.747327924 0.0967893]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 155, loss=5.8013529777526855, loss_1=5.569444179534912, loss_2=0.7382125854492188 (15.995204s)\n",
      "dice[0.894767463 0]\n",
      "dice[0.904398084 0.102748103]\n",
      "dice[0.904398084 0.102748103]\n",
      "dice[0.731293261 0]\n",
      "dice[0.724240363 0]\n",
      "dice[0.724240363 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 156, loss=10.593484878540039, loss_1=5.567153453826904, loss_2=5.567153453826904 (14.030866s)\n",
      "dice[0.886575162 0]\n",
      "dice[0.895624578 0]\n",
      "dice[0.904684663 0.102640927]\n",
      "dice[0.719835579 0]\n",
      "dice[0.727844536 0]\n",
      "dice[0.72440207 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 157, loss=5.567067623138428, loss_1=10.594133377075195, loss_2=10.598397254943848 (12.571493s)\n",
      "dice[0.881192446 0]\n",
      "dice[0.887106955 0]\n",
      "dice[0.894561 0.072328411]\n",
      "dice[0.733795643 0.0920198336]\n",
      "dice[0.747152 0.0973253399]\n",
      "dice[0.748135328 0.0968391821]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 158, loss=0.740960955619812, loss_1=5.589169025421143, loss_2=5.568892002105713 (13.640823s)\n",
      "dice[0.894810736 0.0722495466]\n",
      "dice[0.894810736 0.0722495466]\n",
      "dice[0.904888153 0.102417305]\n",
      "dice[0.724586904 0]\n",
      "dice[0.748314679 0.0968593433]\n",
      "dice[0.748314679 0.0968593433]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 159, loss=5.5670270919799805, loss_1=0.741929292678833, loss_2=0.741929292678833 (15.591306s)\n",
      "dice[0.818315089 0]\n",
      "dice[0.897384942 0]\n",
      "dice[0.905181229 0.102296509]\n",
      "dice[0.870389104 0]\n",
      "dice[0.728767753 0]\n",
      "dice[0.723663449 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 160, loss=10.577823638916016, loss_1=5.5672149658203125, loss_2=10.593461990356445 (14.632886s)\n",
      "dice[0.887752533 0]\n",
      "dice[0.905978858 0.102194227]\n",
      "dice[0.868663311 0.0778701454]\n",
      "dice[0.723996162 0]\n",
      "dice[0.747643769 0.0974069834]\n",
      "dice[0.740013778 0.0928668156]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 161, loss=5.568480014801025, loss_1=0.6902997493743896, loss_2=5.566957950592041 (15.951183s)\n",
      "dice[0.818385184 0]\n",
      "dice[0.896287 0.0720392764]\n",
      "dice[0.87732774 0.0782699659]\n",
      "dice[0.871887863 0]\n",
      "dice[0.711216807 0]\n",
      "dice[0.748997569 0.0968992785]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 162, loss=10.577431678771973, loss_1=0.7442983388900757, loss_2=5.701345443725586 (14.149615s)\n",
      "dice[0.888712645 0]\n",
      "dice[0.888712645 0]\n",
      "dice[0.897155523 0.0719591677]\n",
      "dice[0.748164058 0.0974665284]\n",
      "dice[0.748164058 0.0974665284]\n",
      "dice[0.749208748 0.096944049]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 163, loss=5.568018436431885, loss_1=5.568018436431885, loss_2=0.7450899481773376 (15.866678s)\n",
      "dice[0.818610191 0]\n",
      "dice[0.88737458 0]\n",
      "dice[0.898664713 0]\n",
      "dice[0.873009503 0]\n",
      "dice[0.729246736 0]\n",
      "dice[0.730959237 0.0915496573]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 164, loss=10.577095031738281, loss_1=5.590381145477295, loss_2=10.593021392822266 (14.549533s)\n",
      "dice[0.818059862 0]\n",
      "dice[0.89752239 0.0718149915]\n",
      "dice[0.880280852 0.0783460066]\n",
      "dice[0.707431734 0]\n",
      "dice[0.869146287 0.0867373943]\n",
      "dice[0.749445438 0.0969707742]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 165, loss=5.600488185882568, loss_1=0.7469543218612671, loss_2=5.700709342956543 (15.977919s)\n",
      "dice[0.874491096 0]\n",
      "dice[0.897613645 0.071749717]\n",
      "dice[0.897613645 0.071749717]\n",
      "dice[0.719287336 0]\n",
      "dice[0.749536 0.0969868898]\n",
      "dice[0.749536 0.0969868898]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 166, loss=0.7478177547454834, loss_1=10.601554870605469, loss_2=0.7478177547454834 (17.430997s)\n",
      "dice[0.887235403 0]\n",
      "dice[0.890541792 0]\n",
      "dice[0.890541792 0]\n",
      "dice[0.736455 0.0930757]\n",
      "dice[0.749038637 0.0981649384]\n",
      "dice[0.749038637 0.0981649384]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 167, loss=5.566405773162842, loss_1=5.566405773162842, loss_2=5.582795143127441 (12.832631s)\n",
      "dice[0.81917727 0]\n",
      "dice[0.898712158 0.0716051534]\n",
      "dice[0.898712158 0.0716051534]\n",
      "dice[0.87603116 0]\n",
      "dice[0.750084579 0.0970246345]\n",
      "dice[0.750084579 0.0970246345]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 168, loss=10.57619857788086, loss_1=0.7494233846664429, loss_2=0.7494233846664429 (14.093745s)\n",
      "dice[0.891638041 0]\n",
      "dice[0.900233686 0]\n",
      "dice[0.819119573 0]\n",
      "dice[0.730175436 0]\n",
      "dice[0.876394391 0]\n",
      "dice[0.749616683 0.0983250663]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 169, loss=10.576122283935547, loss_1=5.5658063888549805, loss_2=10.592397689819336 (14.842016s)\n",
      "dice[0.891740501 0]\n",
      "dice[0.818203628 0.0704832822]\n",
      "dice[0.886107445 0.0773136318]\n",
      "dice[0.874794185 0]\n",
      "dice[0.707084835 0]\n",
      "dice[0.74968344 0.0984087661]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 170, loss=5.7769389152526855, loss_1=5.711041450500488, loss_2=5.565674781799316 (16.453466s)\n",
      "dice[0.901685834 0]\n",
      "dice[0.900227904 0.0713903159]\n",
      "dice[0.909452 0.101098448]\n",
      "dice[0.730838597 0]\n",
      "dice[0.725242376 0]\n",
      "dice[0.75072968 0.0970656425]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 171, loss=5.566051483154297, loss_1=0.7519277334213257, loss_2=10.591869354248047 (15.813891s)\n",
      "dice[0.819663 0]\n",
      "dice[0.892073 0]\n",
      "dice[0.892073 0]\n",
      "dice[0.878610313 0]\n",
      "dice[0.749953568 0.0986318961]\n",
      "dice[0.749953568 0.0986318961]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 172, loss=10.575431823730469, loss_1=5.565303325653076, loss_2=5.565303325653076 (14.090995s)\n",
      "dice[0.878765345 0.0761182]\n",
      "dice[0.910335302 0.100896023]\n",
      "dice[0.900598049 0.100889415]\n",
      "dice[0.725521088 0]\n",
      "dice[0.736363828 0.0911441371]\n",
      "dice[0.750961721 0.0970922187]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 173, loss=5.565812110900879, loss_1=0.5397284030914307, loss_2=0.7165937423706055 (15.497532s)\n",
      "dice[0.902845085 0]\n",
      "dice[0.910609 0.100800306]\n",
      "dice[0.901444316 0.100793]\n",
      "dice[0.731327236 0]\n",
      "dice[0.725609601 0]\n",
      "dice[0.75131619 0.0971045271]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 174, loss=10.59145736694336, loss_1=0.5394314527511597, loss_2=5.5657453536987305 (14.028648s)\n",
      "dice[0.874287426 0]\n",
      "dice[0.878466785 0]\n",
      "dice[0.818919837 0.0695637092]\n",
      "dice[0.722314715 0]\n",
      "dice[0.720659792 0]\n",
      "dice[0.878477752 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 175, loss=5.789851188659668, loss_1=10.600217819213867, loss_2=10.600849151611328 (12.586645s)\n",
      "dice[0.894452 0]\n",
      "dice[0.818998694 0.0702039152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.911842942 0.100606345]\n",
      "dice[0.746667385 0.0992359072]\n",
      "dice[0.878762305 0]\n",
      "dice[0.726007521 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 176, loss=5.779960632324219, loss_1=5.565385818481445, loss_2=5.565057277679443 (12.947725s)\n",
      "dice[0.901857495 0.100513838]\n",
      "dice[0.883202136 0.0777554512]\n",
      "dice[0.901857495 0.100513838]\n",
      "dice[0.713511527 0]\n",
      "dice[0.751498044 0.0971492082]\n",
      "dice[0.751498044 0.0971492082]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 177, loss=0.5392770767211914, loss_1=5.705087661743164, loss_2=0.5392770767211914 (13.987303s)\n",
      "dice[0.903352 0]\n",
      "dice[0.894786954 0]\n",
      "dice[0.88753587 0.0770333484]\n",
      "dice[0.731896281 0]\n",
      "dice[0.746955 0.0995736122]\n",
      "dice[0.712527931 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 178, loss=5.564716815948486, loss_1=10.591188430786133, loss_2=5.712592601776123 (16.236058s)\n",
      "dice[0.818961322 0]\n",
      "dice[0.902229965 0.100345343]\n",
      "dice[0.902229965 0.100345343]\n",
      "dice[0.880412579 0]\n",
      "dice[0.750663161 0.097179763]\n",
      "dice[0.750663161 0.097179763]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 179, loss=10.575156211853027, loss_1=0.5393838882446289, loss_2=0.5393838882446289 (14.103613s)\n",
      "dice[0.819213748 0]\n",
      "dice[0.913226664 0.100256175]\n",
      "dice[0.888449073 0.0769016519]\n",
      "dice[0.880753934 0]\n",
      "dice[0.726132 0]\n",
      "dice[0.712786317 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 180, loss=10.575008392333984, loss_1=5.713849067687988, loss_2=5.565096378326416 (14.112720s)\n",
      "dice[0.895956218 0]\n",
      "dice[0.895956218 0]\n",
      "dice[0.818469226 0.0704558641]\n",
      "dice[0.879289508 0]\n",
      "dice[0.747663617 0.100011691]\n",
      "dice[0.747663617 0.100011691]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 181, loss=5.77616024017334, loss_1=5.56409215927124, loss_2=5.56409215927124 (16.185192s)\n",
      "dice[0.90572542 0]\n",
      "dice[0.818762 0.0704818442]\n",
      "dice[0.903362513 0.100082844]\n",
      "dice[0.733031571 0]\n",
      "dice[0.880090356 0]\n",
      "dice[0.750666559 0.0972216427]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 182, loss=5.775496959686279, loss_1=0.5390964150428772, loss_2=10.590311050415039 (15.955391s)\n",
      "dice[0.905814171 0]\n",
      "dice[0.905814171 0]\n",
      "dice[0.913456082 0.0999897]\n",
      "dice[0.733160377 0]\n",
      "dice[0.733160377 0]\n",
      "dice[0.726136744 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 183, loss=5.5651044845581055, loss_1=10.590255737304688, loss_2=10.590255737304688 (12.548155s)\n",
      "dice[0.913727701 0.099917084]\n",
      "dice[0.819464207 0.070237726]\n",
      "dice[0.887426198 0.0770946816]\n",
      "dice[0.726223 0]\n",
      "dice[0.710444331 0]\n",
      "dice[0.876097 0.0855288133]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 184, loss=0.8109700679779053, loss_1=5.712421894073486, loss_2=5.565034866333008 (13.724177s)\n",
      "dice[0.821196795 0.070235081]\n",
      "dice[0.914517939 0.0998219401]\n",
      "dice[0.885711968 0.0772922263]\n",
      "dice[0.726512849 0]\n",
      "dice[0.714790881 0]\n",
      "dice[0.876272 0.0854590237]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 185, loss=0.8110568523406982, loss_1=5.564794540405273, loss_2=5.7094621658325195 (13.542835s)\n",
      "dice[0.819889963 0.0705126524]\n",
      "dice[0.914622247 0.0997238755]\n",
      "dice[0.914622247 0.0997238755]\n",
      "dice[0.881993413 0]\n",
      "dice[0.726490915 0]\n",
      "dice[0.726490915 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 186, loss=5.774277210235596, loss_1=5.564809799194336, loss_2=5.564809799194336 (16.479284s)\n",
      "dice[0.820134163 0]\n",
      "dice[0.905801117 0.0996404365]\n",
      "dice[0.91490227 0.0996284857]\n",
      "dice[0.879042804 0.0852496922]\n",
      "dice[0.726601 0]\n",
      "dice[0.750208616 0.0973257124]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 187, loss=5.608286380767822, loss_1=0.5385763049125671, loss_2=5.564751625061035 (16.090425s)\n",
      "dice[0.875674188 0]\n",
      "dice[0.897703052 0]\n",
      "dice[0.916081071 0.099533096]\n",
      "dice[0.738068521 0.0933674723]\n",
      "dice[0.727028191 0]\n",
      "dice[0.748949826 0.101184957]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 188, loss=5.563040733337402, loss_1=5.584219932556152, loss_2=5.564393997192383 (12.981034s)\n",
      "dice[0.871702373 0]\n",
      "dice[0.820697367 0.0706335083]\n",
      "dice[0.916184843 0.0994358137]\n",
      "dice[0.882677197 0]\n",
      "dice[0.726746678 0]\n",
      "dice[0.741087735 0.094001323]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 189, loss=5.772095680236816, loss_1=5.564487934112549, loss_2=5.582298278808594 (16.242520s)\n",
      "dice[0.899175882 0]\n",
      "dice[0.899175882 0]\n",
      "dice[0.822885931 0.0702808648]\n",
      "dice[0.749669313 0.101506136]\n",
      "dice[0.749669313 0.101506136]\n",
      "dice[0.878523588 0.0851657167]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 190, loss=0.811606764793396, loss_1=5.562412261962891, loss_2=5.562412261962891 (13.766601s)\n",
      "dice[0.899285197 0]\n",
      "dice[0.908479571 0]\n",
      "dice[0.91726321 0.0992491171]\n",
      "dice[0.734235 0]\n",
      "dice[0.749572337 0.101681858]\n",
      "dice[0.727173328 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 191, loss=5.562365531921387, loss_1=5.5642194747924805, loss_2=10.58932113647461 (16.139845s)\n",
      "dice[0.908582211 0]\n",
      "dice[0.908582211 0]\n",
      "dice[0.906646967 0.0991809592]\n",
      "dice[0.734403729 0]\n",
      "dice[0.734403729 0]\n",
      "dice[0.749349415 0.0974085927]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 192, loss=0.5387001037597656, loss_1=10.589253425598145, loss_2=10.589253425598145 (13.755090s)\n",
      "dice[0.900367677 0]\n",
      "dice[0.907449901 0.0990926549]\n",
      "dice[0.881858051 0.0754832402]\n",
      "dice[0.750248671 0.102038838]\n",
      "dice[0.741596818 0.0919712707]\n",
      "dice[0.749483943 0.0975820869]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 193, loss=5.561836242675781, loss_1=0.7186557054519653, loss_2=0.5382652282714844 (15.496045s)\n",
      "dice[0.877417088 0]\n",
      "dice[0.917689323 0.0989875793]\n",
      "dice[0.822684646 0.070573166]\n",
      "dice[0.727396 0]\n",
      "dice[0.885498941 0]\n",
      "dice[0.73922652 0.0935005844]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 194, loss=5.771795749664307, loss_1=5.56423807144165, loss_2=5.583024501800537 (12.934104s)\n",
      "dice[0.900521576 0]\n",
      "dice[0.82297039 0]\n",
      "dice[0.917779744 0.0989109576]\n",
      "dice[0.727456212 0]\n",
      "dice[0.75053966 0.102363408]\n",
      "dice[0.882597625 0.084773086]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 195, loss=5.610379219055176, loss_1=5.564260005950928, loss_2=5.561644077301025 (12.857403s)\n",
      "dice[0.887673497 0]\n",
      "dice[0.918013752 0.0988178849]\n",
      "dice[0.823434949 0.0705634952]\n",
      "dice[0.719985366 0]\n",
      "dice[0.727579534 0]\n",
      "dice[0.886804 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 196, loss=5.771426677703857, loss_1=5.564246654510498, loss_2=10.598085403442383 (12.736774s)\n",
      "dice[0.910506725 0]\n",
      "dice[0.88267225 0]\n",
      "dice[0.909111679 0.0987491459]\n",
      "dice[0.735736549 0]\n",
      "dice[0.726247251 0]\n",
      "dice[0.749242 0.0976376235]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 197, loss=10.588438987731934, loss_1=10.597770690917969, loss_2=0.5381012558937073 (14.617727s)\n",
      "dice[0.911338866 0]\n",
      "dice[0.901700616 0]\n",
      "dice[0.824112356 0.0705436096]\n",
      "dice[0.736185 0]\n",
      "dice[0.887332261 0]\n",
      "dice[0.751361132 0.102868222]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 198, loss=5.771422386169434, loss_1=10.588119506835938, loss_2=5.5610175132751465 (16.339912s)\n",
      "dice[0.901794195 0]\n",
      "dice[0.825256109 0]\n",
      "dice[0.910199642 0.0986021608]\n",
      "dice[0.88969177 0]\n",
      "dice[0.751522958 0.103033692]\n",
      "dice[0.749399602 0.0976730138]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 199, loss=10.57126235961914, loss_1=0.5378736257553101, loss_2=5.560912132263184 (14.222381s)\n",
      "dice[0.902071 0]\n",
      "dice[0.919278085 0.0984913185]\n",
      "dice[0.910286188 0.0985149145]\n",
      "dice[0.728229761 0]\n",
      "dice[0.751751363 0.103193223]\n",
      "dice[0.749303043 0.0976862088]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 200, loss=5.560746192932129, loss_1=5.5640692710876465, loss_2=0.5379421710968018 (15.906655s)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m iter 200 saved: /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models/model.ckpt\n",
      "dice[0.903273 0]\n",
      "dice[0.911716819 0]\n",
      "dice[0.880431 0]\n",
      "dice[0.736664653 0]\n",
      "dice[0.752172291 0.103341311]\n",
      "dice[0.740224957 0.0936013535]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 201, loss=10.587904930114746, loss_1=5.560303688049316, loss_2=5.581671237945557 (14.118704s)\n",
      "dice[0.920562088 0.0983309448]\n",
      "dice[0.827294 0.0708395168]\n",
      "dice[0.91062516 0.0983602181]\n",
      "dice[0.728801 0]\n",
      "dice[0.889142811 0]\n",
      "dice[0.749157786 0.0977120697]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 202, loss=5.765764236450195, loss_1=5.563772678375244, loss_2=0.5380170345306396 (16.076104s)\n",
      "dice[0.912031651 0]\n",
      "dice[0.903596401 0]\n",
      "dice[0.920793295 0.0982604846]\n",
      "dice[0.737034619 0]\n",
      "dice[0.752527237 0.103636391]\n",
      "dice[0.728954136 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 203, loss=10.58773422241211, loss_1=5.560060024261475, loss_2=5.563754558563232 (13.975102s)\n",
      "dice[0.912118196 0]\n",
      "dice[0.890125096 0.0743613]\n",
      "dice[0.920885 0.0981863141]\n",
      "dice[0.737187088 0]\n",
      "dice[0.729046643 0]\n",
      "dice[0.742297828 0.0916079879]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 204, loss=10.587674140930176, loss_1=0.7323441505432129, loss_2=5.563793182373047 (14.032772s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.885557711 0]\n",
      "dice[0.828366756 0]\n",
      "dice[0.911003351 0.0981343612]\n",
      "dice[0.727589786 0]\n",
      "dice[0.887406826 0.0842549354]\n",
      "dice[0.748941839 0.0977867767]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 205, loss=5.611969470977783, loss_1=0.538128137588501, loss_2=10.59671401977539 (15.719262s)\n",
      "dice[0.828656375 0]\n",
      "dice[0.911098123 0.0980607718]\n",
      "dice[0.911098123 0.0980607718]\n",
      "dice[0.887739122 0.0841924101]\n",
      "dice[0.748871207 0.0977988467]\n",
      "dice[0.748871207 0.0977988467]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 206, loss=5.61232328414917, loss_1=0.5381941795349121, loss_2=0.5381941795349121 (15.502476s)\n",
      "dice[0.904813 0]\n",
      "dice[0.913259804 0]\n",
      "dice[0.922003031 0.0979540274]\n",
      "dice[0.737981 0]\n",
      "dice[0.753349125 0.104174361]\n",
      "dice[0.729620278 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 207, loss=10.587190628051758, loss_1=5.563652038574219, loss_2=5.5594162940979 (14.069034s)\n",
      "dice[0.914490163 0]\n",
      "dice[0.922096908 0.097866863]\n",
      "dice[0.91218 0.0978993922]\n",
      "dice[0.738523066 0]\n",
      "dice[0.729728758 0]\n",
      "dice[0.74905628 0.0978271216]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 208, loss=0.5380427837371826, loss_1=10.586746215820312, loss_2=5.563714504241943 (13.865727s)\n",
      "dice[0.922140837 0.0977954268]\n",
      "dice[0.912272811 0.0978289]\n",
      "dice[0.887439549 0.0746524855]\n",
      "dice[0.729810536 0]\n",
      "dice[0.744737566 0.0922106]\n",
      "dice[0.749003291 0.0978388861]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 209, loss=0.5381100177764893, loss_1=0.7260327339172363, loss_2=5.563778400421143 (14.229228s)\n",
      "dice[0.91480422 0]\n",
      "dice[0.830103695 0]\n",
      "dice[0.894303739 0.075849928]\n",
      "dice[0.738668561 0]\n",
      "dice[0.889489532 0.0840600654]\n",
      "dice[0.719461799 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 210, loss=5.612607002258301, loss_1=10.586631774902344, loss_2=5.723402976989746 (16.221519s)\n",
      "dice[0.914894283 0]\n",
      "dice[0.888787687 0]\n",
      "dice[0.913559556 0.0976634249]\n",
      "dice[0.738820672 0]\n",
      "dice[0.724134 0]\n",
      "dice[0.749277472 0.0978749543]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 211, loss=10.586570739746094, loss_1=10.596769332885742, loss_2=0.5378999710083008 (14.532988s)\n",
      "dice[0.906106234 0]\n",
      "dice[0.92328912 0.0975554]\n",
      "dice[0.92328912 0.0975554]\n",
      "dice[0.730449736 0]\n",
      "dice[0.730449736 0]\n",
      "dice[0.753320515 0.104748823]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 212, loss=5.563670635223389, loss_1=5.558956146240234, loss_2=5.563670635223389 (16.256244s)\n",
      "dice[0.830495596 0]\n",
      "dice[0.886500835 0.0746822]\n",
      "dice[0.92337662 0.0974831656]\n",
      "dice[0.895575523 0]\n",
      "dice[0.73055172 0]\n",
      "dice[0.750187933 0.0943776518]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 213, loss=10.5684814453125, loss_1=5.563730716705322, loss_2=0.7167133092880249 (14.063474s)\n",
      "dice[0.90646559 0]\n",
      "dice[0.916028 0]\n",
      "dice[0.923601568 0.0974054411]\n",
      "dice[0.739590108 0]\n",
      "dice[0.753643394 0.104974195]\n",
      "dice[0.730707109 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 214, loss=10.586095809936523, loss_1=5.55872917175293, loss_2=5.563754558563232 (14.044055s)\n",
      "dice[0.916067719 0]\n",
      "dice[0.923700869 0.0973199904]\n",
      "dice[0.89329803 0.0737903193]\n",
      "dice[0.739699543 0]\n",
      "dice[0.730604172 0]\n",
      "dice[0.744367361 0.0917625949]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 215, loss=5.563889503479004, loss_1=0.7378959655761719, loss_2=10.586057662963867 (15.846376s)\n",
      "dice[0.901536465 0]\n",
      "dice[0.907773554 0]\n",
      "dice[0.924903929 0.0972461924]\n",
      "dice[0.745579839 0.0975759923]\n",
      "dice[0.731107652 0]\n",
      "dice[0.754227459 0.105202913]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 216, loss=5.558198928833008, loss_1=5.565295696258545, loss_2=5.563581466674805 (12.984303s)\n",
      "dice[0.897581041 0]\n",
      "dice[0.887780547 0.0744739398]\n",
      "dice[0.915094197 0.0971910432]\n",
      "dice[0.722302139 0]\n",
      "dice[0.750895739 0.0944295377]\n",
      "dice[0.749565721 0.0985908061]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 217, loss=0.5373585820198059, loss_1=10.595029830932617, loss_2=0.7187575101852417 (14.063074s)\n",
      "dice[0.908095956 0]\n",
      "dice[0.83181566 0]\n",
      "dice[0.916468 0]\n",
      "dice[0.740155637 0]\n",
      "dice[0.892518103 0.0837905332]\n",
      "dice[0.754526913 0.105363466]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 218, loss=5.6136555671691895, loss_1=5.5580034255981445, loss_2=10.585844039916992 (12.698054s)\n",
      "dice[0.908884346 0]\n",
      "dice[0.915425777 0.0970278457]\n",
      "dice[0.915425777 0.0970278457]\n",
      "dice[0.754936934 0.105444983]\n",
      "dice[0.749628603 0.0986132249]\n",
      "dice[0.749628603 0.0986132249]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 219, loss=5.55768346786499, loss_1=0.5375153422355652, loss_2=0.5375153422355652 (15.523584s)\n",
      "dice[0.886166692 0]\n",
      "dice[0.908971131 0]\n",
      "dice[0.915515721 0.0969517156]\n",
      "dice[0.755065441 0.105533481]\n",
      "dice[0.742887616 0.0938650519]\n",
      "dice[0.749635398 0.0986251533]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 220, loss=5.557607650756836, loss_1=0.537613570690155, loss_2=5.578680038452148 (15.910692s)\n",
      "dice[0.917549193 0]\n",
      "dice[0.909241438 0]\n",
      "dice[0.874355614 0]\n",
      "dice[0.740838051 0]\n",
      "dice[0.868616343 0]\n",
      "dice[0.7552 0.105612926]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 221, loss=10.585403442382812, loss_1=10.56425666809082, loss_2=5.557486534118652 (14.489215s)\n",
      "dice[0.909278393 0]\n",
      "dice[0.909278393 0]\n",
      "dice[0.836341619 0.0739192]\n",
      "dice[0.755302191 0.105686553]\n",
      "dice[0.755302191 0.105686553]\n",
      "dice[0.823711753 0.0718039647]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 222, loss=5.557433128356934, loss_1=0.9173620343208313, loss_2=5.557433128356934 (16.315069s)\n",
      "dice[0.918925345 0]\n",
      "dice[0.88932693 0]\n",
      "dice[0.916627049 0.0967178643]\n",
      "dice[0.741528511 0]\n",
      "dice[0.73888731 0.092421934]\n",
      "dice[0.749891937 0.0986666679]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 223, loss=10.58488655090332, loss_1=0.5376616716384888, loss_2=5.584197521209717 (14.089743s)\n",
      "dice[0.887769 0]\n",
      "dice[0.91914916 0]\n",
      "dice[0.917869449 0.0966475829]\n",
      "dice[0.741735339 0]\n",
      "dice[0.743454516 0.0939243]\n",
      "dice[0.750299394 0.0986788422]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 224, loss=10.584778785705566, loss_1=5.57794189453125, loss_2=0.5373722314834595 (14.108835s)\n",
      "dice[0.909693956 0]\n",
      "dice[0.919237316 0]\n",
      "dice[0.917961478 0.0965769216]\n",
      "dice[0.741872609 0]\n",
      "dice[0.755776465 0.105945118]\n",
      "dice[0.750313103 0.0986907482]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 225, loss=0.5374723672866821, loss_1=10.584722518920898, loss_2=5.557146072387695 (13.863180s)\n",
      "dice[0.920012534 0]\n",
      "dice[0.920012534 0]\n",
      "dice[0.830364466 0.0709760264]\n",
      "dice[0.742282689 0]\n",
      "dice[0.742282689 0]\n",
      "dice[0.893008769 0.0836976916]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 226, loss=0.8075274229049683, loss_1=10.584425926208496, loss_2=10.584425926208496 (13.877431s)\n",
      "dice[0.920111418 0]\n",
      "dice[0.910565794 0]\n",
      "dice[0.893313706 0.0737196803]\n",
      "dice[0.742441535 0]\n",
      "dice[0.756284237 0.10629151]\n",
      "dice[0.747986138 0.0924556181]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 227, loss=10.584362030029297, loss_1=0.7350244522094727, loss_2=5.5567145347595215 (14.003771s)\n",
      "dice[0.910793602 0]\n",
      "dice[0.910793602 0]\n",
      "dice[0.831280649 0.0711934194]\n",
      "dice[0.756459057 0.10637296]\n",
      "dice[0.898640037 0]\n",
      "dice[0.756459057 0.10637296]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 228, loss=5.757176399230957, loss_1=5.556593894958496, loss_2=5.556593894958496 (16.561393s)\n",
      "dice[0.89915812 0]\n",
      "dice[0.831832945 0]\n",
      "dice[0.897733 0.0730466172]\n",
      "dice[0.725805342 0]\n",
      "dice[0.897256613 0.0833653286]\n",
      "dice[0.746634603 0.0925894901]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 229, loss=5.616064548492432, loss_1=0.7428491711616516, loss_2=10.593759536743164 (15.915973s)\n",
      "dice[0.832435191 0]\n",
      "dice[0.912168443 0]\n",
      "dice[0.919407 0.0961677656]\n",
      "dice[0.901724875 0]\n",
      "dice[0.757097423 0.10653957]\n",
      "dice[0.75081116 0.0987964496]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 230, loss=10.566459655761719, loss_1=0.537738025188446, loss_2=5.556048393249512 (14.143664s)\n",
      "dice[0.920742214 0]\n",
      "dice[0.919499159 0.0960934684]\n",
      "dice[0.919499159 0.0960934684]\n",
      "dice[0.743190944 0]\n",
      "dice[0.750852823 0.0988095328]\n",
      "dice[0.750852823 0.0988095328]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 231, loss=0.5378558039665222, loss_1=10.584016799926758, loss_2=0.5378558039665222 (13.997679s)\n",
      "dice[0.897526681 0]\n",
      "dice[0.832932413 0.0711346194]\n",
      "dice[0.91958797 0.0960188806]\n",
      "dice[0.730968893 0]\n",
      "dice[0.900417328 0]\n",
      "dice[0.750891924 0.0988242701]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 232, loss=5.757181644439697, loss_1=0.5379771590232849, loss_2=10.592876434326172 (15.876208s)\n",
      "dice[0.83387053 0]\n",
      "dice[0.921613872 0]\n",
      "dice[0.906018734 0.103426985]\n",
      "dice[0.90335542 0]\n",
      "dice[0.74377954 0]\n",
      "dice[0.722380042 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 233, loss=10.565693855285645, loss_1=5.567043781280518, loss_2=10.58365249633789 (14.626661s)\n",
      "dice[0.91355896 0]\n",
      "dice[0.919914603 0.0958581865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.919914603 0.0958581865]\n",
      "dice[0.757751703 0.10685122]\n",
      "dice[0.751036942 0.0988514572]\n",
      "dice[0.751036942 0.0988514572]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 234, loss=0.5382031202316284, loss_1=5.555459499359131, loss_2=0.5382031202316284 (17.468808s)\n",
      "dice[0.921843946 0]\n",
      "dice[0.920709133 0.0957744271]\n",
      "dice[0.930847645 0.0957617834]\n",
      "dice[0.744031549 0]\n",
      "dice[0.734082639 0]\n",
      "dice[0.751369536 0.0988735557]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 235, loss=10.58353042602539, loss_1=5.56431770324707, loss_2=0.5380994081497192 (14.165710s)\n",
      "dice[0.832377315 0]\n",
      "dice[0.895785332 0]\n",
      "dice[0.906252742 0.103188969]\n",
      "dice[0.732914329 0]\n",
      "dice[0.899638 0.0890946463]\n",
      "dice[0.722852647 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 236, loss=5.574453830718994, loss_1=10.592824935913086, loss_2=5.5669264793396 (15.977982s)\n",
      "dice[0.832647681 0]\n",
      "dice[0.923234463 0]\n",
      "dice[0.923234463 0]\n",
      "dice[0.744745672 0]\n",
      "dice[0.744745672 0]\n",
      "dice[0.899802744 0.0890676]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 237, loss=5.57450008392334, loss_1=10.58300495147705, loss_2=10.58300495147705 (12.120527s)\n",
      "dice[0.833629668 0.0722709]\n",
      "dice[0.921125472 0.0955415666]\n",
      "dice[0.921125472 0.0955415666]\n",
      "dice[0.903414607 0]\n",
      "dice[0.751377583 0.098913081]\n",
      "dice[0.751377583 0.098913081]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 238, loss=5.739896774291992, loss_1=0.5385253429412842, loss_2=0.5385253429412842 (15.438991s)\n",
      "dice[0.914109051 0]\n",
      "dice[0.922382414 0.0954713076]\n",
      "dice[0.922382414 0.0954713076]\n",
      "dice[0.758329391 0.107233532]\n",
      "dice[0.751837492 0.0989263579]\n",
      "dice[0.751837492 0.0989263579]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 239, loss=5.555082321166992, loss_1=0.5382610559463501, loss_2=0.5382610559463501 (15.432630s)\n",
      "dice[0.914937317 0]\n",
      "dice[0.835933328 0]\n",
      "dice[0.931532204 0.0953649879]\n",
      "dice[0.905672133 0]\n",
      "dice[0.758671343 0.107299946]\n",
      "dice[0.734847844 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 240, loss=10.564598083496094, loss_1=5.564934730529785, loss_2=5.554773330688477 (14.118020s)\n",
      "dice[0.924414396 0]\n",
      "dice[0.915163219 0]\n",
      "dice[0.924414396 0]\n",
      "dice[0.745641053 0]\n",
      "dice[0.745641053 0]\n",
      "dice[0.75882709 0.107360177]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 241, loss=10.582486152648926, loss_1=5.554662227630615, loss_2=10.582486152648926 (14.632967s)\n",
      "dice[0.836455286 0]\n",
      "dice[0.932434797 0.09522181]\n",
      "dice[0.902231574 0.0723546]\n",
      "dice[0.90676868 0]\n",
      "dice[0.735396147 0]\n",
      "dice[0.744719267 0.0927746445]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 242, loss=10.564193725585938, loss_1=0.7510983943939209, loss_2=5.564944267272949 (14.215299s)\n",
      "dice[0.83394748 0.0725240484]\n",
      "dice[0.904913604 0.103610322]\n",
      "dice[0.93265748 0.0951456353]\n",
      "dice[0.725084782 0]\n",
      "dice[0.735576928 0]\n",
      "dice[0.899908721 0.0890490934]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 243, loss=0.7448552250862122, loss_1=5.565046310424805, loss_2=5.566597938537598 (13.585067s)\n",
      "dice[0.924751699 0]\n",
      "dice[0.836259723 0.072515]\n",
      "dice[0.932749927 0.0967297629]\n",
      "dice[0.74610877 0]\n",
      "dice[0.905337453 0]\n",
      "dice[0.735720873 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 244, loss=5.735328197479248, loss_1=10.582284927368164, loss_2=5.561373233795166 (16.303614s)\n",
      "dice[0.837201893 0]\n",
      "dice[0.933988631 0.0966587961]\n",
      "dice[0.933988631 0.0966587961]\n",
      "dice[0.908300102 0]\n",
      "dice[0.736234 0]\n",
      "dice[0.736234 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 245, loss=10.563624382019043, loss_1=5.561070919036865, loss_2=5.561070919036865 (14.091057s)\n",
      "dice[0.836011887 0]\n",
      "dice[0.917660415 0]\n",
      "dice[0.897884548 0.0729560256]\n",
      "dice[0.903232872 0.0886615515]\n",
      "dice[0.759508967 0.107661]\n",
      "dice[0.756097555 0.0948980525]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 246, loss=5.5751633644104, loss_1=5.553792476654053, loss_2=0.733892560005188 (15.967958s)\n",
      "dice[0.835697353 0]\n",
      "dice[0.924222291 0.0948410109]\n",
      "dice[0.924222291 0.0948410109]\n",
      "dice[0.903583884 0.0885747]\n",
      "dice[0.752768219 0.099209249]\n",
      "dice[0.752768219 0.099209249]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 247, loss=5.57567024230957, loss_1=0.5390498638153076, loss_2=0.5390498638153076 (15.467233s)\n",
      "dice[0.92598027 0]\n",
      "dice[0.917992175 0]\n",
      "dice[0.836458206 0.0726480857]\n",
      "dice[0.747055531 0]\n",
      "dice[0.759738266 0.107773557]\n",
      "dice[0.902639091 0.0887798294]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 248, loss=0.7433736324310303, loss_1=10.581741333007812, loss_2=5.553624153137207 (13.928118s)\n",
      "dice[0.901604772 0]\n",
      "dice[0.918086767 0]\n",
      "dice[0.912243783 0.101646572]\n",
      "dice[0.730906129 0]\n",
      "dice[0.759822071 0.10853371]\n",
      "dice[0.720553815 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 249, loss=5.553389549255371, loss_1=10.591872215270996, loss_2=5.5663886070251465 (16.416568s)\n",
      "dice[0.834307373 0]\n",
      "dice[0.918176115 0]\n",
      "dice[0.918176115 0]\n",
      "dice[0.905181348 0.0884662792]\n",
      "dice[0.759898484 0.108599484]\n",
      "dice[0.759898484 0.108599484]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 250, loss=5.576267719268799, loss_1=5.55333137512207, loss_2=5.55333137512207 (16.361853s)\n",
      "dice[0.898116529 0]\n",
      "dice[0.918405235 0]\n",
      "dice[0.837382615 0.0724293813]\n",
      "dice[0.760028839 0.108660042]\n",
      "dice[0.743269444 0.0957354754]\n",
      "dice[0.908675075 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 251, loss=5.73541259765625, loss_1=5.553226470947266, loss_2=5.570265769958496 (16.499234s)\n",
      "dice[0.918505907 0]\n",
      "dice[0.927861 0]\n",
      "dice[0.926717639 0.0944043472]\n",
      "dice[0.748060524 0]\n",
      "dice[0.760105848 0.108725153]\n",
      "dice[0.753987789 0.099290818]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 252, loss=10.58102035522461, loss_1=0.5393534302711487, loss_2=5.553165435791016 (14.153903s)\n",
      "dice[0.919287324 0]\n",
      "dice[0.935873091 0.0960176066]\n",
      "dice[0.935873091 0.0960176066]\n",
      "dice[0.760465682 0.108777404]\n",
      "dice[0.737871468 0]\n",
      "dice[0.737871468 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 253, loss=5.5528669357299805, loss_1=5.561524391174316, loss_2=5.561524391174316 (16.449772s)\n",
      "dice[0.928759933 0]\n",
      "dice[0.83522743 0.0727782845]\n",
      "dice[0.935960889 0.0959454551]\n",
      "dice[0.748652756 0]\n",
      "dice[0.738021 0]\n",
      "dice[0.904856086 0.0884422287]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 254, loss=0.7433249354362488, loss_1=5.5616278648376465, loss_2=10.580646514892578 (13.810897s)\n",
      "dice[0.893960059 0]\n",
      "dice[0.927146316 0.0941573]\n",
      "dice[0.927146316 0.0941573]\n",
      "dice[0.749691367 0.0959813744]\n",
      "dice[0.75437516 0.0993840843]\n",
      "dice[0.75437516 0.0993840843]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 255, loss=0.5398633480072021, loss_1=5.569129467010498, loss_2=0.5398633480072021 (17.858819s)\n",
      "dice[0.905974329 0]\n",
      "dice[0.929085493 0]\n",
      "dice[0.83715117 0.0728570446]\n",
      "dice[0.735589802 0]\n",
      "dice[0.749053597 0]\n",
      "dice[0.91065377 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 256, loss=5.7290191650390625, loss_1=10.580465316772461, loss_2=10.589609146118164 (12.524245s)\n",
      "dice[0.929130375 0]\n",
      "dice[0.921031177 0]\n",
      "dice[0.928207874 0.0939916372]\n",
      "dice[0.749207377 0]\n",
      "dice[0.761212766 0.108999602]\n",
      "dice[0.754072487 0.0994179174]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 257, loss=5.552188873291016, loss_1=0.5401873588562012, loss_2=10.580415725708008 (15.857931s)\n",
      "dice[0.921296299 0]\n",
      "dice[0.899171472 0]\n",
      "dice[0.833235383 0.0727851614]\n",
      "dice[0.761369705 0.109047323]\n",
      "dice[0.748964429 0.0982933193]\n",
      "dice[0.90660882 0.0882057697]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 258, loss=0.7447290420532227, loss_1=5.564120769500732, loss_2=5.552072048187256 (13.985845s)\n",
      "dice[0.922077537 0]\n",
      "dice[0.929485738 0]\n",
      "dice[0.835466564 0.0724085048]\n",
      "dice[0.749534369 0]\n",
      "dice[0.761685312 0.109097175]\n",
      "dice[0.912135959 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 259, loss=5.735320091247559, loss_1=5.551784992218018, loss_2=10.580245018005371 (12.813915s)\n",
      "dice[0.930252194 0]\n",
      "dice[0.822238445 0]\n",
      "dice[0.938666284 0.0954652652]\n",
      "dice[0.749977827 0]\n",
      "dice[0.842296839 0]\n",
      "dice[0.739572108 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 260, loss=10.583866119384766, loss_1=10.57994270324707, loss_2=5.561715126037598 (14.786762s)\n",
      "dice[0.896552444 0]\n",
      "dice[0.938751698 0.0953953341]\n",
      "dice[0.938751698 0.0953953341]\n",
      "dice[0.750679314 0.0974528491]\n",
      "dice[0.739736676 0]\n",
      "dice[0.739736676 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 261, loss=5.561830043792725, loss_1=5.561830043792725, loss_2=5.565450668334961 (12.849122s)\n",
      "dice[0.821325243 0.0727870762]\n",
      "dice[0.929519415 0.0935803354]\n",
      "dice[0.929519415 0.0935803354]\n",
      "dice[0.841444254 0]\n",
      "dice[0.755023599 0.0995119]\n",
      "dice[0.755023599 0.0995119]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 262, loss=5.751246929168701, loss_1=0.5409537553787231, loss_2=0.5409537553787231 (15.457748s)\n",
      "dice[0.931913078 0]\n",
      "dice[0.915308058 0.100841828]\n",
      "dice[0.90787667 0.100586593]\n",
      "dice[0.750937104 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice[0.72699964 0]\n",
      "dice[0.748275697 0.0946598202]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 263, loss=10.579286575317383, loss_1=5.564212799072266, loss_2=0.5442796945571899 (14.032210s)\n",
      "dice[0.929858863 0.093407914]\n",
      "dice[0.820008218 0.0727128834]\n",
      "dice[0.939825237 0.0951598659]\n",
      "dice[0.841825426 0]\n",
      "dice[0.740499258 0]\n",
      "dice[0.754895926 0.0995457]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 264, loss=5.752510070800781, loss_1=5.561985492706299, loss_2=0.5414884090423584 (16.190773s)\n",
      "dice[0.905080259 0]\n",
      "dice[0.939909637 0.0950929448]\n",
      "dice[0.929951072 0.0941068307]\n",
      "dice[0.738488 0]\n",
      "dice[0.740665 0]\n",
      "dice[0.755027175 0.0995631292]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 265, loss=0.5390680432319641, loss_1=10.589107513427734, loss_2=5.562102794647217 (13.830401s)\n",
      "dice[0.93232286 0]\n",
      "dice[0.823111832 0]\n",
      "dice[0.904894054 0.10128212]\n",
      "dice[0.751507401 0]\n",
      "dice[0.84525919 0]\n",
      "dice[0.753073394 0.0952531099]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 266, loss=10.582906723022461, loss_1=10.579042434692383, loss_2=0.542007565498352 (14.500215s)\n",
      "dice[0.82349962 0]\n",
      "dice[0.931468546 0.093959339]\n",
      "dice[0.940210819 0.0949313715]\n",
      "dice[0.846178949 0]\n",
      "dice[0.741037428 0]\n",
      "dice[0.755685508 0.0995963961]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 267, loss=10.58258056640625, loss_1=0.5389856696128845, loss_2=5.5623779296875 (14.061870s)\n",
      "dice[0.933175147 0]\n",
      "dice[0.823584437 0]\n",
      "dice[0.931557715 0.0938911811]\n",
      "dice[0.751296818 0]\n",
      "dice[0.846477151 0]\n",
      "dice[0.755807519 0.099615857]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 268, loss=10.582484245300293, loss_1=0.539148211479187, loss_2=10.578882217407227 (14.770053s)\n",
      "dice[0.932327926 0.09382727]\n",
      "dice[0.932327926 0.09382727]\n",
      "dice[0.932327926 0.09382727]\n",
      "dice[0.756203234 0.0996324718]\n",
      "dice[0.756203234 0.0996324718]\n",
      "dice[0.756203234 0.0996324718]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 269, loss=0.5390616655349731, loss_1=0.5390616655349731, loss_2=0.5390616655349731 (14.282545s)\n",
      "dice[0.91065383 0]\n",
      "dice[0.925446 0]\n",
      "dice[0.823625743 0]\n",
      "dice[0.738344848 0]\n",
      "dice[0.8468979 0]\n",
      "dice[0.763467252 0.109589949]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 270, loss=10.582368850708008, loss_1=5.550374507904053, loss_2=10.587749481201172 (14.717661s)\n",
      "dice[0.933558404 0]\n",
      "dice[0.925670743 0]\n",
      "dice[0.932644188 0.0936836153]\n",
      "dice[0.75178957 0]\n",
      "dice[0.763624072 0.109619513]\n",
      "dice[0.75645709 0.0996887609]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 271, loss=10.578662872314453, loss_1=0.5393799543380737, loss_2=5.550271511077881 (14.099121s)\n",
      "dice[0.941373 0.0945717916]\n",
      "dice[0.932726204 0.0936206]\n",
      "dice[0.919013143 0.100079581]\n",
      "dice[0.741967916 0]\n",
      "dice[0.728524625 0]\n",
      "dice[0.756558836 0.0997039303]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 272, loss=5.562888145446777, loss_1=5.563095569610596, loss_2=0.5395437479019165 (15.382810s)\n",
      "dice[0.926502287 0]\n",
      "dice[0.942629218 0.0944842324]\n",
      "dice[0.932762325 0.0935552418]\n",
      "dice[0.764024615 0.109875217]\n",
      "dice[0.742524385 0]\n",
      "dice[0.756610334 0.0997198746]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 273, loss=0.5397413969039917, loss_1=5.562696933746338, loss_2=5.549899578094482 (13.455075s)\n",
      "dice[0.934618056 0]\n",
      "dice[0.934618056 0]\n",
      "dice[0.823610306 0]\n",
      "dice[0.752509594 0]\n",
      "dice[0.752509594 0]\n",
      "dice[0.848299325 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 274, loss=10.582022666931152, loss_1=10.578218460083008, loss_2=10.578218460083008 (15.104224s)\n",
      "dice[0.942906857 0.0943668112]\n",
      "dice[0.820068955 0.0731546283]\n",
      "dice[0.933058858 0.0934255943]\n",
      "dice[0.739814103 0]\n",
      "dice[0.846413314 0]\n",
      "dice[0.756830215 0.0997536108]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 275, loss=5.7452592849731445, loss_1=0.5400537848472595, loss_2=5.563661098480225 (15.978672s)\n",
      "dice[0.901898444 0]\n",
      "dice[0.926891 0]\n",
      "dice[0.942978561 0.0942970067]\n",
      "dice[0.739945292 0]\n",
      "dice[0.76436764 0.109951869]\n",
      "dice[0.753042817 0.100259073]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 276, loss=5.549697399139404, loss_1=5.563826084136963, loss_2=5.561200141906738 (16.397960s)\n",
      "dice[0.902177036 0]\n",
      "dice[0.926927745 0]\n",
      "dice[0.820424 0.0727497041]\n",
      "dice[0.847430229 0]\n",
      "dice[0.764451146 0.109977081]\n",
      "dice[0.753158033 0.100420974]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 277, loss=5.75049352645874, loss_1=5.561061382293701, loss_2=5.549661159515381 (16.273699s)\n",
      "dice[0.927192807 0]\n",
      "dice[0.936330676 0]\n",
      "dice[0.820384204 0.0727417544]\n",
      "dice[0.753451526 0]\n",
      "dice[0.764628589 0.110014103]\n",
      "dice[0.847676814 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 278, loss=5.750552177429199, loss_1=10.577554702758789, loss_2=5.549540996551514 (12.802788s)\n",
      "dice[0.824997663 0]\n",
      "dice[0.934190273 0.0931530297]\n",
      "dice[0.944006503 0.0940909386]\n",
      "dice[0.850813389 0]\n",
      "dice[0.7406708 0]\n",
      "dice[0.757342458 0.0998734534]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 279, loss=10.581047058105469, loss_1=5.564037322998047, loss_2=0.5405844449996948 (14.117611s)\n",
      "dice[0.928041875 0]\n",
      "dice[0.944081903 0.0940223113]\n",
      "dice[0.934260607 0.09309]\n",
      "dice[0.740805268 0]\n",
      "dice[0.765071213 0.110062145]\n",
      "dice[0.757448733 0.0998936296]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 280, loss=5.549206256866455, loss_1=5.564205646514893, loss_2=0.5407665967941284 (15.881775s)\n",
      "dice[0.913860381 0]\n",
      "dice[0.821567535 0.07297685]\n",
      "dice[0.909342587 0.100107297]\n",
      "dice[0.740514278 0]\n",
      "dice[0.843611777 0.0867694]\n",
      "dice[0.75101769 0.0954588428]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 281, loss=0.7700934410095215, loss_1=0.5411739349365234, loss_2=10.586406707763672 (17.604699s)\n",
      "dice[0.827137291 0]\n",
      "dice[0.928354442 0]\n",
      "dice[0.935765266 0.0929599106]\n",
      "dice[0.851125658 0]\n",
      "dice[0.765350819 0.110114336]\n",
      "dice[0.758135438 0.0999382213]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 282, loss=10.580434799194336, loss_1=5.549045085906982, loss_2=0.5406919717788696 (14.127638s)\n",
      "dice[0.827295661 0]\n",
      "dice[0.923781216 0.0987904593]\n",
      "dice[0.935843468 0.092897065]\n",
      "dice[0.851449609 0]\n",
      "dice[0.725653231 0]\n",
      "dice[0.75824964 0.0999607518]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 283, loss=10.580313682556152, loss_1=5.563309669494629, loss_2=0.5408756136894226 (14.013444s)\n",
      "dice[0.93778789 0]\n",
      "dice[0.929751158 0]\n",
      "dice[0.93658042 0.0928294584]\n",
      "dice[0.754686177 0]\n",
      "dice[0.765952528 0.110229902]\n",
      "dice[0.758592546 0.0999821126]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 284, loss=5.548516273498535, loss_1=0.5408581495285034, loss_2=10.576881408691406 (15.913949s)\n",
      "dice[0.936655283 0.0927655]\n",
      "dice[0.945209086 0.0936531126]\n",
      "dice[0.945209086 0.0936531126]\n",
      "dice[0.741746783 0]\n",
      "dice[0.741746783 0]\n",
      "dice[0.758703411 0.100024663]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 285, loss=0.5410472750663757, loss_1=5.564918518066406, loss_2=5.564918518066406 (13.558031s)\n",
      "dice[0.937906325 0]\n",
      "dice[0.937906325 0]\n",
      "dice[0.820424557 0.0728083551]\n",
      "dice[0.754928231 0]\n",
      "dice[0.754928231 0]\n",
      "dice[0.850625753 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 286, loss=5.748881816864014, loss_1=10.576791763305664, loss_2=10.576791763305664 (12.595498s)\n",
      "dice[0.930147171 0]\n",
      "dice[0.938169241 0]\n",
      "dice[0.930147171 0]\n",
      "dice[0.755132079 0]\n",
      "dice[0.766346574 0.110352494]\n",
      "dice[0.766346574 0.110352494]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 287, loss=10.576675415039062, loss_1=5.548288345336914, loss_2=5.548288345336914 (14.076583s)\n",
      "dice[0.918015242 0]\n",
      "dice[0.930900514 0]\n",
      "dice[0.936975241 0.0925565213]\n",
      "dice[0.736982942 0]\n",
      "dice[0.766731441 0.110379338]\n",
      "dice[0.759020865 0.100087591]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 288, loss=5.547996997833252, loss_1=10.586250305175781, loss_2=0.5416913032531738 (15.723128s)\n",
      "dice[0.930983901 0]\n",
      "dice[0.909713 0]\n",
      "dice[0.946834743 0.0933744833]\n",
      "dice[0.765963495 0.110401928]\n",
      "dice[0.742763877 0]\n",
      "dice[0.753643811 0.10359478]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 289, loss=5.548162460327148, loss_1=5.5652313232421875, loss_2=5.558261871337891 (16.233839s)\n",
      "dice[0.829860568 0]\n",
      "dice[0.93903327 0]\n",
      "dice[0.919755042 0.0996459499]\n",
      "dice[0.854277134 0]\n",
      "dice[0.75551188 0]\n",
      "dice[0.732511818 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 290, loss=10.57896614074707, loss_1=5.562053203582764, loss_2=10.576363563537598 (14.589590s)\n",
      "dice[0.931285083 0]\n",
      "dice[0.931285083 0]\n",
      "dice[0.947114766 0.0932408944]\n",
      "dice[0.74306196 0]\n",
      "dice[0.766232729 0.110444859]\n",
      "dice[0.766232729 0.110444859]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 291, loss=5.548008918762207, loss_1=5.548008918762207, loss_2=5.5655670166015625 (12.831566s)\n",
      "dice[0.826440573 0]\n",
      "dice[0.938082814 0.0923221856]\n",
      "dice[0.938082814 0.0923221856]\n",
      "dice[0.849228621 0.0863692909]\n",
      "dice[0.759766817 0.100178868]\n",
      "dice[0.759766817 0.100178868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 292, loss=5.6059393882751465, loss_1=0.542149543762207, loss_2=0.542149543762207 (15.417948s)\n",
      "dice[0.826749206 0]\n",
      "dice[0.938155293 0.0922572613]\n",
      "dice[0.947932184 0.0931247324]\n",
      "dice[0.849291325 0.0863583535]\n",
      "dice[0.743568361 0]\n",
      "dice[0.759874403 0.10041935]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 293, loss=5.605924129486084, loss_1=5.5656609535217285, loss_2=0.5423109531402588 (16.073283s)\n",
      "dice[0.917933524 0]\n",
      "dice[0.948004544 0.0930538699]\n",
      "dice[0.948004544 0.0930538699]\n",
      "dice[0.742329597 0]\n",
      "dice[0.743695855 0]\n",
      "dice[0.743695855 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 294, loss=5.565873622894287, loss_1=5.565873622894287, loss_2=10.58493423461914 (12.729937s)\n",
      "dice[0.931704104 0]\n",
      "dice[0.822971106 0.0727941766]\n",
      "dice[0.913710415 0.0990509689]\n",
      "dice[0.85363239 0]\n",
      "dice[0.766675591 0.110543296]\n",
      "dice[0.750442863 0.0956653655]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 295, loss=5.747689723968506, loss_1=0.5402050614356995, loss_2=5.547769546508789 (15.948095s)\n",
      "dice[0.912875772 0]\n",
      "dice[0.932464838 0]\n",
      "dice[0.823298335 0.0727751255]\n",
      "dice[0.853962541 0]\n",
      "dice[0.767065287 0.110564716]\n",
      "dice[0.750399292 0.104396515]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 296, loss=5.74778938293457, loss_1=5.547476291656494, loss_2=5.558082103729248 (16.377278s)\n",
      "dice[0.932729721 0]\n",
      "dice[0.948328316 0.0928385258]\n",
      "dice[0.939777911 0.0919931]\n",
      "dice[0.743972421 0]\n",
      "dice[0.767257631 0.110587835]\n",
      "dice[0.760795176 0.100541323]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 297, loss=5.547356128692627, loss_1=5.566536903381348, loss_2=0.5427507162094116 (15.983143s)\n",
      "dice[0.932814658 0]\n",
      "dice[0.919300139 0]\n",
      "dice[0.948408544 0.0927494764]\n",
      "dice[0.743056357 0]\n",
      "dice[0.767375708 0.110609628]\n",
      "dice[0.744115591 0]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 298, loss=5.547299861907959, loss_1=5.566823959350586, loss_2=10.584410667419434 (16.185455s)\n",
      "dice[0.942057073 0]\n",
      "dice[0.940072596 0.0918443203]\n",
      "dice[0.940072596 0.0918443203]\n",
      "dice[0.75738579 0]\n",
      "dice[0.757730544 0.100613102]\n",
      "dice[0.757730544 0.100613102]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 299, loss=10.575139999389648, loss_1=0.5440636873245239, loss_2=0.5440636873245239 (14.135086s)\n",
      "dice[0.831285834 0]\n",
      "dice[0.920636773 0]\n",
      "dice[0.948699892 0.0926080942]\n",
      "dice[0.743383 0]\n",
      "dice[0.744439423 0]\n",
      "dice[0.85104847 0.0863769725]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m training iter 300, loss=5.60421895980835, loss_1=10.58399486541748, loss_2=5.56722354888916 (12.836107s)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m iter 300 saved: /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models/model.ckpt\n",
      "\u001b[1mINFO:niftynet:\u001b[0m cleaning up...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m stopping sampling threads\n",
      "\u001b[1mINFO:niftynet:\u001b[0m iter 300 saved: /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models/model.ckpt\n",
      "\u001b[1mINFO:niftynet:\u001b[0m SegmentationApplication stopped (time in second 4460.69).\n"
     ]
    }
   ],
   "source": [
    "! net_segment train -c /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/config.ini --name dense_vnet_TC_craneal.CNN_TC_craneal_ictus.MyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t\t    model.ckpt-200.data-00000-of-00001\r\n",
      "model.ckpt-100.data-00000-of-00001  model.ckpt-200.index\r\n",
      "model.ckpt-100.index\t\t    model.ckpt-200.meta\r\n",
      "model.ckpt-100.meta\t\t    model.ckpt-300.data-00000-of-00001\r\n",
      "model.ckpt-1.data-00000-of-00001    model.ckpt-300.index\r\n",
      "model.ckpt-1.index\t\t    model.ckpt-300.meta\r\n",
      "model.ckpt-1.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL:tensorflow:Optional Python module cv2 not found, please install cv2 and retry if the application fails.\n",
      "CRITICAL:tensorflow:Optional Python module skimage.io not found, please install skimage.io and retry if the application fails.\n",
      "CRITICAL:tensorflow:Optional Python module SimpleITK not found, please install SimpleITK and retry if the application fails.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module yaml not found, please install yaml and retry if the application fails.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module yaml version None not found, please install yaml-None and retry if the application fails.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module SimpleITK not found, please install SimpleITK and retry if the application fails.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module SimpleITK version None not found, please install SimpleITK-None and retry if the application fails.\n",
      "NiftyNet version 0.5.0\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module yaml not found, please install yaml and retry if the application fails.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module yaml version None not found, please install yaml-None and retry if the application fails.\n",
      "[CUSTOM]\n",
      "-- num_classes: 2\n",
      "-- output_prob: False\n",
      "-- label_normalisation: False\n",
      "-- softmax: True\n",
      "-- min_sampling_ratio: 0\n",
      "-- compulsory_labels: (0, 1)\n",
      "-- rand_samples: 0\n",
      "-- min_numb_labels: 1\n",
      "-- proba_connect: True\n",
      "-- evaluation_units: foreground\n",
      "-- label: ('label',)\n",
      "-- inferred: ()\n",
      "-- sampler: ()\n",
      "-- weight: ()\n",
      "-- image: ('ct',)\n",
      "-- name: net_segment\n",
      "[CONFIG_FILE]\n",
      "-- path: /home/bsc0001/niftynet/extensions/dense_vnet_TC_craneal/config.ini\n",
      "[CT]\n",
      "-- csv_file: \n",
      "-- path_to_search: ./data/dense_vnet_TC_craneal/\n",
      "-- filename_contains: ('TC',)\n",
      "-- filename_not_contains: ()\n",
      "-- filename_removefromid: \n",
      "-- interp_order: 1\n",
      "-- loader: None\n",
      "-- pixdim: ()\n",
      "-- axcodes: ('A', 'R', 'S')\n",
      "-- spatial_window_size: (512, 512, 40)\n",
      "[LABEL]\n",
      "-- csv_file: \n",
      "-- path_to_search: ./data/dense_vnet_TC_craneal/\n",
      "-- filename_contains: ('mask',)\n",
      "-- filename_not_contains: ()\n",
      "-- filename_removefromid: \n",
      "-- interp_order: 0\n",
      "-- loader: None\n",
      "-- pixdim: ()\n",
      "-- axcodes: ('A', 'R', 'S')\n",
      "-- spatial_window_size: (512, 512, 40)\n",
      "[SYSTEM]\n",
      "-- cuda_devices: \"\"\n",
      "-- num_threads: 1\n",
      "-- num_gpus: 1\n",
      "-- model_dir: /home/bsc0001/niftynet/models/dense_vnet_TC_craneal\n",
      "-- dataset_split_file: ./dataset_split.csv\n",
      "-- event_handler: ('model_saver', 'model_restorer', 'sampler_threading', 'apply_gradients', 'output_interpreter', 'console_logger', 'tensorboard_logger')\n",
      "-- iteration_generator: iteration_generator\n",
      "-- action: inference\n",
      "[NETWORK]\n",
      "-- name: dense_vnet\n",
      "-- activation_function: relu\n",
      "-- batch_size: 2\n",
      "-- smaller_final_batch_mode: pad\n",
      "-- decay: 0.0\n",
      "-- reg_type: L2\n",
      "-- volume_padding_size: (0, 0, 0)\n",
      "-- volume_padding_mode: minimum\n",
      "-- window_sampling: resize\n",
      "-- queue_length: 12\n",
      "-- multimod_foreground_type: and\n",
      "-- histogram_ref_file: ./histogram_ref_file.txt\n",
      "-- norm_type: percentile\n",
      "-- cutoff: (0.01, 0.99)\n",
      "-- foreground_type: otsu_plus\n",
      "-- normalisation: False\n",
      "-- whitening: False\n",
      "-- normalise_foreground_only: False\n",
      "-- weight_initializer: he_normal\n",
      "-- bias_initializer: zeros\n",
      "-- keep_prob: 1.0\n",
      "[TRAINING]\n",
      "-- optimiser: adam\n",
      "-- sample_per_volume: 1\n",
      "-- rotation_angle: ()\n",
      "-- rotation_angle_x: ()\n",
      "-- rotation_angle_y: ()\n",
      "-- rotation_angle_z: ()\n",
      "-- scaling_percentage: ()\n",
      "-- antialiasing: True\n",
      "-- bias_field_range: ()\n",
      "-- bf_order: 3\n",
      "-- random_flipping_axes: -1\n",
      "-- do_elastic_deformation: False\n",
      "-- num_ctrl_points: 4\n",
      "-- deformation_sigma: 15\n",
      "-- proportion_to_deform: 0.5\n",
      "-- lr: 0.001\n",
      "-- loss_type: dense_vnet_TC_craneal.dice_hinge.dice\n",
      "-- starting_iter: 0\n",
      "-- save_every_n: 100\n",
      "-- tensorboard_every_n: 20\n",
      "-- max_iter: 300\n",
      "-- max_checkpoints: 100\n",
      "-- validation_every_n: -1\n",
      "-- validation_max_iter: 1\n",
      "-- exclude_fraction_for_validation: 0.0\n",
      "-- exclude_fraction_for_inference: 0.0\n",
      "-- vars_to_restore: \n",
      "-- vars_to_freeze: \n",
      "[INFERENCE]\n",
      "-- spatial_window_size: (512, 512, 40)\n",
      "-- inference_iter: 300\n",
      "-- dataset_to_infer: \n",
      "-- save_seg_dir: ./segmentation_output/\n",
      "-- output_postfix: _niftynet_out\n",
      "-- output_interp_order: 0\n",
      "-- border: (0, 0, 0)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m starting segmentation application\n",
      "\u001b[1mINFO:niftynet:\u001b[0m `csv_file = ` not found, writing to \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/ct.csv\" instead.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Overwriting existing: \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/ct.csv\".\n",
      "\u001b[1mINFO:niftynet:\u001b[0m [ct] search file folders, writing csv file /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/ct.csv\n",
      "\u001b[1mINFO:niftynet:\u001b[0m `csv_file = ` not found, writing to \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/label.csv\" instead.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Overwriting existing: \"/home/bsc0001/niftynet/models/dense_vnet_TC_craneal/label.csv\".\n",
      "\u001b[1mINFO:niftynet:\u001b[0m [label] search file folders, writing csv file /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/label.csv\n",
      "\u001b[1mINFO:niftynet:\u001b[0m \n",
      "\n",
      "Number of subjects 2, input section names: ['subject_id', 'ct', 'label']\n",
      "-- using all subjects (without data partitioning).\n",
      "\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 2 subjects from sections ('ct',) as input [image]\n",
      "2019-06-18 13:36:28.921477: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-18 13:36:30.268504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:05:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.74GiB\n",
      "2019-06-18 13:36:30.451368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:06:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.75GiB\n",
      "2019-06-18 13:36:30.588835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 2 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.75GiB\n",
      "2019-06-18 13:36:30.592448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 13:36:31.364467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 13:36:31.364501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 13:36:31.364508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 13:36:31.364512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 13:36:31.364516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 13:36:31.365228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 13:36:31.518062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 13:36:31.670889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m reading size of preprocessed images\n",
      "\u001b[1mINFO:niftynet:\u001b[0m initialised resize sampler {'image': (1, 512, 512, 40, 1, 1), 'image_location': (1, 7)} \n",
      "\u001b[1mWARNING:niftynet:\u001b[0m From /home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "\u001b[1mINFO:niftynet:\u001b[0m using DenseVNet\n",
      "2019-06-18 13:36:31.879896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 13:36:31.880038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 13:36:31.880048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 13:36:31.880055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 13:36:31.880061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 13:36:31.880066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 13:36:31.880375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 13:36:31.880635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 13:36:31.880812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initialising Dataset from 2 subjects...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 13:36:34.437734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2\n",
      "2019-06-18 13:36:34.437875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-18 13:36:34.437886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 \n",
      "2019-06-18 13:36:34.437892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y \n",
      "2019-06-18 13:36:34.437898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y \n",
      "2019-06-18 13:36:34.437903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N \n",
      "2019-06-18 13:36:34.438214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2019-06-18 13:36:34.438450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11363 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2019-06-18 13:36:34.438628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11363 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Restoring parameters from /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models/model.ckpt-300\n",
      "2019-06-18 13:36:34.800331: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key DenseVNet/batch_norm/DenseVNet/batch_norm/moving_mean/biased not found in checkpoint\n",
      "\u001b[1mCRITICAL:niftynet:\u001b[0m checkpoint /home/bsc0001/niftynet/models/dense_vnet_TC_craneal/models/model.ckpt-300 not found or variables to restore do not match the current application graph\n",
      "\u001b[1mINFO:niftynet:\u001b[0m cleaning up...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m stopping sampling threads\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Key DenseVNet/batch_norm/DenseVNet/batch_norm/moving_mean/biased not found in checkpoint\n",
      "\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1538, in restore\n",
      "    {self.saver_def.filename_tensor_name: save_path})\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 887, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Key DenseVNet/batch_norm/DenseVNet/batch_norm/moving_mean/biased not found in checkpoint\n",
      "\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "Caused by op 'save/RestoreV2', defined at:\n",
      "  File \"/home/bsc0001/niftynet/bin/net_segment\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/__init__.py\", line 142, in main\n",
      "    app_driver.run(app_driver.app)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/application_driver.py\", line 197, in run\n",
      "    SESS_STARTED.send(application, iter_msg=None)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/blinker/base.py\", line 267, in send\n",
      "    for receiver in self.receivers_for(sender)]\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/blinker/base.py\", line 267, in <listcomp>\n",
      "    for receiver in self.receivers_for(sender)]\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/handler_model.py\", line 109, in restore_model\n",
      "    var_list=to_restore, save_relative_paths=True)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\n",
      "    self.build()\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1106, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1143, in _build\n",
      "    build_save=build_save, build_restore=build_restore)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 787, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 854, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n",
      "    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "NotFoundError (see above for traceback): Key DenseVNet/batch_norm/DenseVNet/batch_norm/moving_mean/biased not found in checkpoint\n",
      "\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1548, in restore\n",
      "    names_to_keys = object_graph_key_mapping(save_path)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1822, in object_graph_key_mapping\n",
      "    checkpointable.OBJECT_GRAPH_PROTO_KEY)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 359, in get_tensor\n",
      "    status)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 526, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsc0001/niftynet/bin/net_segment\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/__init__.py\", line 142, in main\n",
      "    app_driver.run(app_driver.app)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/application_driver.py\", line 197, in run\n",
      "    SESS_STARTED.send(application, iter_msg=None)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/blinker/base.py\", line 267, in send\n",
      "    for receiver in self.receivers_for(sender)]\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/blinker/base.py\", line 267, in <listcomp>\n",
      "    for receiver in self.receivers_for(sender)]\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/handler_model.py\", line 110, in restore_model\n",
      "    saver.restore(tf.get_default_session(), checkpoint)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1554, in restore\n",
      "    err, \"a Variable name or other graph key that is missing\")\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "Key DenseVNet/batch_norm/DenseVNet/batch_norm/moving_mean/biased not found in checkpoint\n",
      "\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "Caused by op 'save/RestoreV2', defined at:\n",
      "  File \"/home/bsc0001/niftynet/bin/net_segment\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/__init__.py\", line 142, in main\n",
      "    app_driver.run(app_driver.app)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/application_driver.py\", line 197, in run\n",
      "    SESS_STARTED.send(application, iter_msg=None)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/blinker/base.py\", line 267, in send\n",
      "    for receiver in self.receivers_for(sender)]\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/blinker/base.py\", line 267, in <listcomp>\n",
      "    for receiver in self.receivers_for(sender)]\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/niftynet/engine/handler_model.py\", line 109, in restore_model\n",
      "    var_list=to_restore, save_relative_paths=True)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\n",
      "    self.build()\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1106, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1143, in _build\n",
      "    build_save=build_save, build_restore=build_restore)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 787, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 854, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n",
      "    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/bsc0001/niftynet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "Key DenseVNet/batch_norm/DenseVNet/batch_norm/moving_mean/biased not found in checkpoint\n",
      "\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Segmentar utilizando los datos del entrenamiento, desde la configuración inicial.\n",
    "! net_segment inference -c ~/niftynet/extensions/dense_vnet_TC_craneal/config.ini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
